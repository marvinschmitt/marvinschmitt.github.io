[
  {
    "objectID": "speaking/index.html",
    "href": "speaking/index.html",
    "title": "Public Speaking, Talks, and Presentations",
    "section": "",
    "text": "Here’s a list of my selected public speaking occasions, talks, and presentations where I was involved. If available, you’ll also find slides, related papers, and recordings. Bold font indicates presenter.\n\nSchmitt, M. (2023). Talk: Towards Reliable Amortized Bayesian Inference. First presented at the Bayes on the Beach 2024 conference.\nSchmitt, M., Bürkner, P.-C. (2024). Workshop: Amortized Bayesian Inference with BayesFlow. First presented at the Bayes on the Beach 2024 conference.\nSchmitt, M., Habermann, D., Bürkner, P. C., & Radev, S. T. (2023). Talk: Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. First presented at the NeurIPS Workshop for Unifying Representations in Neural Models. Preprint available on ArXiv.\nSchmitt, M. (2023). Research talk (invited): Jointly Amortized Bayesian Inference, Machine and Human Intelligence group, University of Helsinki & Finnish Center for Artificial Intelligence (FCAI), Finland\nSchmitt, M., Bürkner, P. C., Köthe, U., & Radev, S. T. (2023). Talk (oral): Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks. First presented at the German Conference on Pattern Recognition. Paper published in GCPR Conference Proceedings (best paper honorable mention). Previously featured also as a poster and short talk.\nSchmitt, M. (2023). Talk (contributed): Amortized Simulation‑Based Inference, Tooling Session, ELLIS Doctoral Symposium, Helsinki, Finland\nSchmitt, M., Radev, S. T., & Bürkner, P. C. (2023). Poster: Meta-Uncertainty in Bayesian Model Comparison. First presented at AISTATS 2023. Paper published in AISSTATS Conference Proceedings.\n\n\n\nSchmitt, M. (2023). Talk (invited): What is AI?, Writing workshop for AI short stories, Cyber Valley, Tübingen, Germany\nSchmitt, M. (2023). Talk (invited): Where Does AI begin?, Cyber Valley Office hours (general public audience), Tübingen, Germany\nSchmitt, M. (2023). Talk (invited): Validating synthetic training data in probabilistic machine learning, Blue Yonder Group Inc., Karlsruhe, Germany"
  },
  {
    "objectID": "impressum/index.html",
    "href": "impressum/index.html",
    "title": "Impressum",
    "section": "",
    "text": "Kontakt:\nmail /dott/ marvinschmitt /ett/ gmail /dott/ com\n\n\nVerantwortlich für den Inhalt nach § 55 Abs. 2 RStV:\nMarvin Schmitt \n\n\nHaftungsausschluss:\nHaftung für Inhalte Die Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\nHaftung für Links Unser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\nUrheberrecht Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\nDatenschutz Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben. Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich. Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Dritte zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.\nGoogle Analytics Diese Website benutzt Google Analytics, einen Webanalysedienst der Google Inc. (‘’Google’‘). Google Analytics verwendet sog.’‘Cookies’’, Textdateien, die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglicht. Die durch den Cookie erzeugten Informationen über Ihre Benutzung dieser Website (einschließlich Ihrer IP-Adresse) wird an einen Server von Google in den USA übertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website auszuwerten, um Reports über die Websiteaktivitäten für die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Sie können die Installation der Cookies durch eine entsprechende Einstellung Ihrer Browser Software verhindern; wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website voll umfänglich nutzen können. Durch die Nutzung dieser Website erklären Sie sich mit der Bearbeitung der über Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.\nGoogle AdSense Diese Website benutzt Google Adsense, einen Webanzeigendienst der Google Inc., USA (‘’Google’‘). Google Adsense verwendet sog.’‘Cookies’’ (Textdateien), die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglicht. Google Adsense verwendet auch sog. ‘’Web Beacons’’ (kleine unsichtbare Grafiken) zur Sammlung von Informationen. Durch die Verwendung des Web Beacons können einfache Aktionen wie der Besucherverkehr auf der Webseite aufgezeichnet und gesammelt werden. Die durch den Cookie und/oder Web Beacon erzeugten Informationen über Ihre Benutzung dieser Website (einschließlich Ihrer IP-Adresse) werden an einen Server von Google in den USA übertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website im Hinblick auf die Anzeigen auszuwerten, um Reports über die Websiteaktivitäten und Anzeigen für die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Das Speichern von Cookies auf Ihrer Festplatte und die Anzeige von Web Beacons können Sie verhindern, indem Sie in Ihren Browser-Einstellungen ‘’keine Cookies akzeptieren’’ wählen (Im MS Internet-Explorer unter ‘’Extras &gt; Internetoptionen &gt; Datenschutz &gt; Einstellung’‘; im Firefox unter’‘Extras &gt; Einstellungen &gt; Datenschutz &gt; Cookies’’); wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website voll umfänglich nutzen können. Durch die Nutzung dieser Website erklären Sie sich mit der Bearbeitung der über Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.\nDiese Webseite nutzt Affiliate Links über das Amazon Partnerprogramm.\n\nDatenschutz­erklärung\n\n\n\nDatenschutz auf einen Blick\n\n\nAllgemeine Hinweise\n\n\nDie folgenden Hinweise geben einen einfachen Überblick darüber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie persönlich identifiziert werden können. Ausführliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgeführten Datenschutzerklärung.\n\n\nDatenerfassung auf dieser Website\n\n\nWer ist verantwortlich für die Datenerfassung auf dieser Website?\n\n\nDie Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten können Sie dem Abschnitt „Hinweis zur Verantwortlichen Stelle“ in dieser Datenschutzerklärung entnehmen.\n\n\nWie erfassen wir Ihre Daten?\n\n\nIhre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben.\n\n\nAndere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.\n\n\nWofür nutzen wir Ihre Daten?\n\n\nEin Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gewährleisten. Andere Daten können zur Analyse Ihres Nutzerverhaltens verwendet werden.\n\n\nWelche Rechte haben Sie bezüglich Ihrer Daten?\n\n\nSie haben jederzeit das Recht, unentgeltlich Auskunft über Herkunft, Empfänger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben außerdem ein Recht, die Berichtigung oder Löschung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, können Sie diese Einwilligung jederzeit für die Zukunft widerrufen. Außerdem haben Sie das Recht, unter bestimmten Umständen die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu.\n\n\nHierzu sowie zu weiteren Fragen zum Thema Datenschutz können Sie sich jederzeit an uns wenden.\n\n\nAnalyse-Tools und Tools von Dritt­anbietern\n\n\nBeim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen.\n\n\nDetaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerklärung.\n\n\n\nHosting\n\n\nWir hosten die Inhalte unserer Website bei folgendem Anbieter:\n\n\nExternes Hosting\n\n\nDiese Website wird extern gehostet. Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters / der Hoster gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die über eine Website generiert werden, handeln.\n\n\nDas externe Hosting erfolgt zum Zwecke der Vertragserfüllung gegenüber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\n\n\nUnser(e) Hoster wird bzw. werden Ihre Daten nur insoweit verarbeiten, wie dies zur Erfüllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.\n\n\nWir setzen folgende(n) Hoster ein:\n\n\nNetlify\n\n\n\nAllgemeine Hinweise und Pflicht­informationen\n\n\nDatenschutz\n\n\nDie Betreiber dieser Seiten nehmen den Schutz Ihrer persönlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend den gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerklärung.\n\n\nWenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie persönlich identifiziert werden können. Die vorliegende Datenschutzerklärung erläutert, welche Daten wir erheben und wofür wir sie nutzen. Sie erläutert auch, wie und zu welchem Zweck das geschieht.\n\n\nWir weisen darauf hin, dass die Datenübertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich.\n\n\nHinweis zur verantwortlichen Stelle\n\n\nDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\n\n\n[Voller Namen bzw. die vollständige Unternehmensbezeichnung des Website-Betreibers sowie die vollständige Anschrift]\n\n\nTelefon: [Telefonnummer der verantwortlichen Stelle] E-Mail: mail //dot__ marvinschmitt __at// gmail _dot/ com\n\n\nVerantwortliche Stelle ist die natürliche oder juristische Person, die allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. Ä.) entscheidet.\n\n\n\n\n\nSpeicherdauer\n\n\nSoweit innerhalb dieser Datenschutzerklärung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck für die Datenverarbeitung entfällt. Wenn Sie ein berechtigtes Löschersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gelöscht, sofern wir keine anderen rechtlich zulässigen Gründe für die Speicherung Ihrer personenbezogenen Daten haben (z. B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die Löschung nach Fortfall dieser Gründe.\n\n\nAllgemeine Hinweise zu den Rechtsgrundlagen der Datenverarbeitung auf dieser Website\n\n\nSofern Sie in die Datenverarbeitung eingewilligt haben, verarbeiten wir Ihre personenbezogenen Daten auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO bzw. Art. 9 Abs. 2 lit. a DSGVO, sofern besondere Datenkategorien nach Art. 9 Abs. 1 DSGVO verarbeitet werden. Im Falle einer ausdrücklichen Einwilligung in die Übertragung personenbezogener Daten in Drittstaaten erfolgt die Datenverarbeitung außerdem auf Grundlage von Art. 49 Abs. 1 lit. a DSGVO. Sofern Sie in die Speicherung von Cookies oder in den Zugriff auf Informationen in Ihr Endgerät (z. B. via Device-Fingerprinting) eingewilligt haben, erfolgt die Datenverarbeitung zusätzlich auf Grundlage von § 25 Abs. 1 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Sind Ihre Daten zur Vertragserfüllung oder zur Durchführung vorvertraglicher Maßnahmen erforderlich, verarbeiten wir Ihre Daten auf Grundlage des Art. 6 Abs. 1 lit. b DSGVO. Des Weiteren verarbeiten wir Ihre Daten, sofern diese zur Erfüllung einer rechtlichen Verpflichtung erforderlich sind auf Grundlage von Art. 6 Abs. 1 lit. c DSGVO. Die Datenverarbeitung kann ferner auf Grundlage unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO erfolgen. Über die jeweils im Einzelfall einschlägigen Rechtsgrundlagen wird in den folgenden Absätzen dieser Datenschutzerklärung informiert.\n\n\nHinweis zur Datenweitergabe in die USA und sonstige Drittstaaten\n\n\nWir verwenden unter anderem Tools von Unternehmen mit Sitz in den USA oder sonstigen datenschutzrechtlich nicht sicheren Drittstaaten. Wenn diese Tools aktiv sind, können Ihre personenbezogene Daten in diese Drittstaaten übertragen und dort verarbeitet werden. Wir weisen darauf hin, dass in diesen Ländern kein mit der EU vergleichbares Datenschutzniveau garantiert werden kann. Beispielsweise sind US-Unternehmen dazu verpflichtet, personenbezogene Daten an Sicherheitsbehörden herauszugeben, ohne dass Sie als Betroffener hiergegen gerichtlich vorgehen könnten. Es kann daher nicht ausgeschlossen werden, dass US-Behörden (z. B. Geheimdienste) Ihre auf US-Servern befindlichen Daten zu Überwachungszwecken verarbeiten, auswerten und dauerhaft speichern. Wir haben auf diese Verarbeitungstätigkeiten keinen Einfluss.\n\n\nWiderruf Ihrer Einwilligung zur Datenverarbeitung\n\n\nViele Datenverarbeitungsvorgänge sind nur mit Ihrer ausdrücklichen Einwilligung möglich. Sie können eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\n\n\nWiderspruchsrecht gegen die Datenerhebung in besonderen Fällen sowie gegen Direktwerbung (Art. 21 DSGVO)\n\n\nWENN DIE DATENVERARBEITUNG AUF GRUNDLAGE VON ART. 6 ABS. 1 LIT. E ODER F DSGVO ERFOLGT, HABEN SIE JEDERZEIT DAS RECHT, AUS GRÜNDEN, DIE SICH AUS IHRER BESONDEREN SITUATION ERGEBEN, GEGEN DIE VERARBEITUNG IHRER PERSONENBEZOGENEN DATEN WIDERSPRUCH EINZULEGEN; DIES GILT AUCH FÜR EIN AUF DIESE BESTIMMUNGEN GESTÜTZTES PROFILING. DIE JEWEILIGE RECHTSGRUNDLAGE, AUF DENEN EINE VERARBEITUNG BERUHT, ENTNEHMEN SIE DIESER DATENSCHUTZERKLÄRUNG. WENN SIE WIDERSPRUCH EINLEGEN, WERDEN WIR IHRE BETROFFENEN PERSONENBEZOGENEN DATEN NICHT MEHR VERARBEITEN, ES SEI DENN, WIR KÖNNEN ZWINGENDE SCHUTZWÜRDIGE GRÜNDE FÜR DIE VERARBEITUNG NACHWEISEN, DIE IHRE INTERESSEN, RECHTE UND FREIHEITEN ÜBERWIEGEN ODER DIE VERARBEITUNG DIENT DER GELTENDMACHUNG, AUSÜBUNG ODER VERTEIDIGUNG VON RECHTSANSPRÜCHEN (WIDERSPRUCH NACH ART. 21 ABS. 1 DSGVO).\n\n\nWERDEN IHRE PERSONENBEZOGENEN DATEN VERARBEITET, UM DIREKTWERBUNG ZU BETREIBEN, SO HABEN SIE DAS RECHT, JEDERZEIT WIDERSPRUCH GEGEN DIE VERARBEITUNG SIE BETREFFENDER PERSONENBEZOGENER DATEN ZUM ZWECKE DERARTIGER WERBUNG EINZULEGEN; DIES GILT AUCH FÜR DAS PROFILING, SOWEIT ES MIT SOLCHER DIREKTWERBUNG IN VERBINDUNG STEHT. WENN SIE WIDERSPRECHEN, WERDEN IHRE PERSONENBEZOGENEN DATEN ANSCHLIESSEND NICHT MEHR ZUM ZWECKE DER DIREKTWERBUNG VERWENDET (WIDERSPRUCH NACH ART. 21 ABS. 2 DSGVO).\n\n\nBeschwerde­recht bei der zuständigen Aufsichts­behörde\n\n\nIm Falle von Verstößen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbehörde, insbesondere in dem Mitgliedstaat ihres gewöhnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutmaßlichen Verstoßes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.\n\n\nRecht auf Daten­übertrag­barkeit\n\n\nSie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem gängigen, maschinenlesbaren Format aushändigen zu lassen. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\n\n\nAuskunft, Löschung und Berichtigung\n\n\nSie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder Löschung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit an uns wenden.\n\n\nRecht auf Einschränkung der Verarbeitung\n\n\nSie haben das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu können Sie sich jederzeit an uns wenden. Das Recht auf Einschränkung der Verarbeitung besteht in folgenden Fällen:\n\n\n\nWenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, benötigen wir in der Regel Zeit, um dies zu überprüfen. Für die Dauer der Prüfung haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\n\nWenn die Verarbeitung Ihrer personenbezogenen Daten unrechtmäßig geschah/geschieht, können Sie statt der Löschung die Einschränkung der Datenverarbeitung verlangen.\n\n\nWenn wir Ihre personenbezogenen Daten nicht mehr benötigen, Sie sie jedoch zur Ausübung, Verteidigung oder Geltendmachung von Rechtsansprüchen benötigen, haben Sie das Recht, statt der Löschung die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\n\nWenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abwägung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen überwiegen, haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\n\n\nWenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschränkt haben, dürfen diese Daten – von ihrer Speicherung abgesehen – nur mit Ihrer Einwilligung oder zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person oder aus Gründen eines wichtigen öffentlichen Interesses der Europäischen Union oder eines Mitgliedstaats verarbeitet werden.\n\n\nSSL- bzw. TLS-Verschlüsselung\n\n\nDiese Seite nutzt aus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS-Verschlüsselung. Eine verschlüsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von „http://“ auf „https://“ wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.\n\n\nWenn die SSL- bzw. TLS-Verschlüsselung aktiviert ist, können die Daten, die Sie an uns übermitteln, nicht von Dritten mitgelesen werden.\n\n\n\nDatenerfassung auf dieser Website\n\n\nCookies\n\n\nUnsere Internetseiten verwenden so genannte „Cookies“. Cookies sind kleine Datenpakete und richten auf Ihrem Endgerät keinen Schaden an. Sie werden entweder vorübergehend für die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endgerät gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gelöscht. Permanente Cookies bleiben auf Ihrem Endgerät gespeichert, bis Sie diese selbst löschen oder eine automatische Löschung durch Ihren Webbrowser erfolgt.\n\n\nCookies können von uns (First-Party-Cookies) oder von Drittunternehmen stammen (sog. Third-Party-Cookies). Third-Party-Cookies ermöglichen die Einbindung bestimmter Dienstleistungen von Drittunternehmen innerhalb von Webseiten (z. B. Cookies zur Abwicklung von Zahlungsdienstleistungen).\n\n\nCookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Webseitenfunktionen ohne diese nicht funktionieren würden (z. B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies können zur Auswertung des Nutzerverhaltens oder zu Werbezwecken verwendet werden.\n\n\nCookies, die zur Durchführung des elektronischen Kommunikationsvorgangs, zur Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z. B. für die Warenkorbfunktion) oder zur Optimierung der Website (z. B. Cookies zur Messung des Webpublikums) erforderlich sind (notwendige Cookies), werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von notwendigen Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies und vergleichbaren Wiedererkennungstechnologien abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG); die Einwilligung ist jederzeit widerrufbar.\n\n\nSie können Ihren Browser so einstellen, dass Sie über das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies für bestimmte Fälle oder generell ausschließen sowie das automatische Löschen der Cookies beim Schließen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalität dieser Website eingeschränkt sein.\n\n\nWelche Cookies und Dienste auf dieser Website eingesetzt werden, können Sie dieser Datenschutzerklärung entnehmen.\n\n\nKontaktformular\n\n\nWenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und für den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter.\n\n\nDie Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erfüllung eines Vertrags zusammenhängt oder zur Durchführung vorvertraglicher Maßnahmen erforderlich ist. In allen übrigen Fällen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde; die Einwilligung ist jederzeit widerrufbar.\n\n\nDie von Ihnen im Kontaktformular eingegebenen Daten verbleiben bei uns, bis Sie uns zur Löschung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck für die Datenspeicherung entfällt (z. B. nach abgeschlossener Bearbeitung Ihrer Anfrage). Zwingende gesetzliche Bestimmungen – insbesondere Aufbewahrungsfristen – bleiben unberührt.\n\n\n\nSoziale Medien\n\n\nFacebook\n\n\nAuf dieser Website sind Elemente des sozialen Netzwerks Facebook integriert. Anbieter dieses Dienstes ist die Meta Platforms Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland. Die erfassten Daten werden nach Aussage von Facebook jedoch auch in die USA und in andere Drittländer übertragen.\n\n\nEine Übersicht über die Facebook Social-Media-Elemente finden Sie hier: https://developers.facebook.com/docs/plugins/?locale=de_DE.\n\n\nWenn das Social-Media-Element aktiv ist, wird eine direkte Verbindung zwischen Ihrem Endgerät und dem Facebook-Server hergestellt. Facebook erhält dadurch die Information, dass Sie mit Ihrer IP-Adresse diese Website besucht haben. Wenn Sie den Facebook „Like-Button“ anklicken, während Sie in Ihrem Facebook-Account eingeloggt sind, können Sie die Inhalte dieser Website auf Ihrem Facebook-Profil verlinken. Dadurch kann Facebook den Besuch dieser Website Ihrem Benutzerkonto zuordnen. Wir weisen darauf hin, dass wir als Anbieter der Seiten keine Kenntnis vom Inhalt der übermittelten Daten sowie deren Nutzung durch Facebook erhalten. Weitere Informationen hierzu finden Sie in der Datenschutzerklärung von Facebook unter: https://de-de.facebook.com/privacy/explanation.\n\n\nSoweit eine Einwilligung (Consent) eingeholt wurde, erfolgt der Einsatz des o. g. Dienstes auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Soweit keine Einwilligung eingeholt wurde, erfolgt die Verwendung des Dienstes auf Grundlage unseres berechtigten Interesses an einer möglichst umfassenden Sichtbarkeit in den Sozialen Medien.\n\n\nSoweit mit Hilfe des hier beschriebenen Tools personenbezogene Daten auf unserer Website erfasst und an Facebook weitergeleitet werden, sind wir und die Meta Platforms Ireland Limited, 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Irland gemeinsam für diese Datenverarbeitung verantwortlich (Art. 26 DSGVO). Die gemeinsame Verantwortlichkeit beschränkt sich dabei ausschließlich auf die Erfassung der Daten und deren Weitergabe an Facebook. Die nach der Weiterleitung erfolgende Verarbeitung durch Facebook ist nicht Teil der gemeinsamen Verantwortung. Die uns gemeinsam obliegenden Verpflichtungen wurden in einer Vereinbarung über gemeinsame Verarbeitung festgehalten. Den Wortlaut der Vereinbarung finden Sie unter: https://www.facebook.com/legal/controller_addendum. Laut dieser Vereinbarung sind wir für die Erteilung der Datenschutzinformationen beim Einsatz des Facebook-Tools und für die datenschutzrechtlich sichere Implementierung des Tools auf unserer Website verantwortlich. Für die Datensicherheit der Facebook-Produkte ist Facebook verantwortlich. Betroffenenrechte (z. B. Auskunftsersuchen) hinsichtlich der bei Facebook verarbeiteten Daten können Sie direkt bei Facebook geltend machen. Wenn Sie die Betroffenenrechte bei uns geltend machen, sind wir verpflichtet, diese an Facebook weiterzuleiten.\n\n\nDie Datenübertragung in die USA wird auf die Standardvertragsklauseln der EU-Kommission gestützt. Details finden Sie hier: https://www.facebook.com/legal/EU_data_transfer_addendum, https://de-de.facebook.com/help/566994660333381 und https://www.facebook.com/policy.php.\n\n\nTwitter\n\n\nAuf dieser Website sind Funktionen des Dienstes Twitter eingebunden. Diese Funktionen werden angeboten durch die Twitter International Company, One Cumberland Place, Fenian Street, Dublin 2, D02 AX07, Irland.\n\n\nWenn das Social-Media-Element aktiv ist, wird eine direkte Verbindung zwischen Ihrem Endgerät und dem Twitter-Server hergestellt. Twitter erhält dadurch Informationen über den Besuch dieser Website durch Sie. Durch das Benutzen von Twitter und der Funktion „Re-Tweet“ werden die von Ihnen besuchten Websites mit Ihrem Twitter-Account verknüpft und anderen Nutzern bekannt gegeben. Wir weisen darauf hin, dass wir als Anbieter der Seiten keine Kenntnis vom Inhalt der übermittelten Daten sowie deren Nutzung durch Twitter erhalten. Weitere Informationen hierzu finden Sie in der Datenschutzerklärung von Twitter unter: https://twitter.com/de/privacy.\n\n\nSoweit eine Einwilligung (Consent) eingeholt wurde, erfolgt der Einsatz des o. g. Dienstes auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Soweit keine Einwilligung eingeholt wurde, erfolgt die Verwendung des Dienstes auf Grundlage unseres berechtigten Interesses an einer möglichst umfassenden Sichtbarkeit in den Sozialen Medien.\n\n\nDie Datenübertragung in die USA wird auf die Standardvertragsklauseln der EU-Kommission gestützt. Details finden Sie hier: https://gdpr.twitter.com/en/controller-to-controller-transfers.html.\n\n\nIhre Datenschutzeinstellungen bei Twitter können Sie in den Konto-Einstellungen unter https://twitter.com/account/settings ändern.\n\n\n\nNewsletter\n\n\nNewsletter­daten\n\n\nWenn Sie den auf der Website angebotenen Newsletter beziehen möchten, benötigen wir von Ihnen eine E-Mail-Adresse sowie Informationen, welche uns die Überprüfung gestatten, dass Sie der Inhaber der angegebenen E-Mail-Adresse sind und mit dem Empfang des Newsletters einverstanden sind. Weitere Daten werden nicht bzw. nur auf freiwilliger Basis erhoben. Diese Daten verwenden wir ausschließlich für den Versand der angeforderten Informationen und geben diese nicht an Dritte weiter.\n\n\nDie Verarbeitung der in das Newsletteranmeldeformular eingegebenen Daten erfolgt ausschließlich auf Grundlage Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO). Die erteilte Einwilligung zur Speicherung der Daten, der E-Mail-Adresse sowie deren Nutzung zum Versand des Newsletters können Sie jederzeit widerrufen, etwa über den „Austragen“-Link im Newsletter. Die Rechtmäßigkeit der bereits erfolgten Datenverarbeitungsvorgänge bleibt vom Widerruf unberührt.\n\n\nDie von Ihnen zum Zwecke des Newsletter-Bezugs bei uns hinterlegten Daten werden von uns bis zu Ihrer Austragung aus dem Newsletter bei uns bzw. dem Newsletterdiensteanbieter gespeichert und nach der Abbestellung des Newsletters oder nach Zweckfortfall aus der Newsletterverteilerliste gelöscht. Wir behalten uns vor, E-Mail-Adressen aus unserem Newsletterverteiler nach eigenem Ermessen im Rahmen unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO zu löschen oder zu sperren.\n\n\nDaten, die zu anderen Zwecken bei uns gespeichert wurden, bleiben hiervon unberührt.\n\n\nNach Ihrer Austragung aus der Newsletterverteilerliste wird Ihre E-Mail-Adresse bei uns bzw. dem Newsletterdiensteanbieter ggf. in einer Blacklist gespeichert, sofern dies zur Verhinderung künftiger Mailings erforderlich ist. Die Daten aus der Blacklist werden nur für diesen Zweck verwendet und nicht mit anderen Daten zusammengeführt. Dies dient sowohl Ihrem Interesse als auch unserem Interesse an der Einhaltung der gesetzlichen Vorgaben beim Versand von Newslettern (berechtigtes Interesse im Sinne des Art. 6 Abs. 1 lit. f DSGVO). Die Speicherung in der Blacklist ist zeitlich nicht befristet. Sie können der Speicherung widersprechen, sofern Ihre Interessen unser berechtigtes Interesse überwiegen.\n\n\n\nPlugins und Tools\n\n\nYouTube mit erweitertem Datenschutz\n\n\nDiese Website bindet Videos der Website YouTube ein. Betreiber der Seiten ist die Google Ireland Limited („Google“), Gordon House, Barrow Street, Dublin 4, Irland.\n\n\nWir nutzen YouTube im erweiterten Datenschutzmodus. Dieser Modus bewirkt laut YouTube, dass YouTube keine Informationen über die Besucher auf dieser Website speichert, bevor diese sich das Video ansehen. Die Weitergabe von Daten an YouTube-Partner wird durch den erweiterten Datenschutzmodus hingegen nicht zwingend ausgeschlossen. So stellt YouTube – unabhängig davon, ob Sie sich ein Video ansehen – eine Verbindung zum Google DoubleClick-Netzwerk her.\n\n\nSobald Sie ein YouTube-Video auf dieser Website starten, wird eine Verbindung zu den Servern von YouTube hergestellt. Dabei wird dem YouTube-Server mitgeteilt, welche unserer Seiten Sie besucht haben. Wenn Sie in Ihrem YouTube-Account eingeloggt sind, ermöglichen Sie YouTube, Ihr Surfverhalten direkt Ihrem persönlichen Profil zuzuordnen. Dies können Sie verhindern, indem Sie sich aus Ihrem YouTube-Account ausloggen.\n\n\nDes Weiteren kann YouTube nach Starten eines Videos verschiedene Cookies auf Ihrem Endgerät speichern oder vergleichbare Wiedererkennungstechnologien (z. B. Device-Fingerprinting) einsetzen. Auf diese Weise kann YouTube Informationen über Besucher dieser Website erhalten. Diese Informationen werden u. a. verwendet, um Videostatistiken zu erfassen, die Anwenderfreundlichkeit zu verbessern und Betrugsversuchen vorzubeugen.\n\n\nGegebenenfalls können nach dem Start eines YouTube-Videos weitere Datenverarbeitungsvorgänge ausgelöst werden, auf die wir keinen Einfluss haben.\n\n\nDie Nutzung von YouTube erfolgt im Interesse einer ansprechenden Darstellung unserer Online-Angebote. Dies stellt ein berechtigtes Interesse im Sinne von Art. 6 Abs. 1 lit. f DSGVO dar. Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\n\n\nWeitere Informationen über Datenschutz bei YouTube finden Sie in deren Datenschutzerklärung unter: https://policies.google.com/privacy?hl=de.\n\n\nGoogle Fonts (lokales Hosting)\n\n\nDiese Seite nutzt zur einheitlichen Darstellung von Schriftarten so genannte Google Fonts, die von Google bereitgestellt werden. Die Google Fonts sind lokal installiert. Eine Verbindung zu Servern von Google findet dabei nicht statt.\n\n\nWeitere Informationen zu Google Fonts finden Sie unter https://developers.google.com/fonts/faq und in der Datenschutzerklärung von Google: https://policies.google.com/privacy?hl=de.\n\n\nFont Awesome (lokales Hosting)\n\n\nDiese Seite nutzt zur einheitlichen Darstellung von Schriftarten Font Awesome. Font Awesome ist lokal installiert. Eine Verbindung zu Servern von Fonticons, Inc. findet dabei nicht statt.\n\n\nWeitere Informationen zu Font Awesome finden Sie in der Datenschutzerklärung für Font Awesome unter: https://fontawesome.com/privacy.\n\n\nGoogle Maps\n\n\nDiese Seite nutzt den Kartendienst Google Maps. Anbieter ist die Google Ireland Limited („Google“), Gordon House, Barrow Street, Dublin 4, Irland.\n\n\nZur Nutzung der Funktionen von Google Maps ist es notwendig, Ihre IP-Adresse zu speichern. Diese Informationen werden in der Regel an einen Server von Google in den USA übertragen und dort gespeichert. Der Anbieter dieser Seite hat keinen Einfluss auf diese Datenübertragung. Wenn Google Maps aktiviert ist, kann Google zum Zwecke der einheitlichen Darstellung der Schriftarten Google Fonts verwenden. Beim Aufruf von Google Maps lädt Ihr Browser die benötigten Web Fonts in ihren Browsercache, um Texte und Schriftarten korrekt anzuzeigen.\n\n\nDie Nutzung von Google Maps erfolgt im Interesse einer ansprechenden Darstellung unserer Online-Angebote und an einer leichten Auffindbarkeit der von uns auf der Website angegebenen Orte. Dies stellt ein berechtigtes Interesse im Sinne von Art. 6 Abs. 1 lit. f DSGVO dar. Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\n\n\nDie Datenübertragung in die USA wird auf die Standardvertragsklauseln der EU-Kommission gestützt. Details finden Sie hier: https://privacy.google.com/businesses/gdprcontrollerterms/ und https://privacy.google.com/businesses/gdprcontrollerterms/sccs/.\n\n\nMehr Informationen zum Umgang mit Nutzerdaten finden Sie in der Datenschutzerklärung von Google: https://policies.google.com/privacy?hl=de.\n\n\nOpenStreetMap\n\n\nWir nutzen den Kartendienst von OpenStreetMap (OSM).\n\n\nWir binden das Kartenmaterial von OpenStreetMap auf dem Server der OpenStreetMap Foundation, St John’s Innovation Centre, Cowley Road, Cambridge, CB4 0WS, Großbritannien, ein. Großbritannien gilt als datenschutzrechtlich sicherer Drittstaat. Das bedeutet, dass Großbritannien ein Datenschutzniveau aufweist, das dem Datenschutzniveau in der Europäischen Union entspricht. Bei der Nutzung der OpenStreetMap-Karten wird eine Verbindung zu den Servern der OpenStreetMap-Foundation hergestellt. Dabei können u. a. Ihre IP-Adresse und weitere Informationen über Ihr Verhalten auf dieser Website an die OSMF weitergeleitet werden. OpenStreetMap speichert hierzu unter Umständen Cookies in Ihrem Browser oder setzt vergleichbare Wiedererkennungstechnologien ein.\n\n\nDie Nutzung von OpenStreetMap erfolgt im Interesse einer ansprechenden Darstellung unserer Online-Angebote und einer leichten Auffindbarkeit der von uns auf der Website angegebenen Orte. Dies stellt ein berechtigtes Interesse im Sinne von Art. 6 Abs. 1 lit. f DSGVO dar. Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\n\n\nGoogle reCAPTCHA\n\n\nWir nutzen „Google reCAPTCHA“ (im Folgenden „reCAPTCHA“) auf dieser Website. Anbieter ist die Google Ireland Limited („Google“), Gordon House, Barrow Street, Dublin 4, Irland.\n\n\nMit reCAPTCHA soll überprüft werden, ob die Dateneingabe auf dieser Website (z. B. in einem Kontaktformular) durch einen Menschen oder durch ein automatisiertes Programm erfolgt. Hierzu analysiert reCAPTCHA das Verhalten des Websitebesuchers anhand verschiedener Merkmale. Diese Analyse beginnt automatisch, sobald der Websitebesucher die Website betritt. Zur Analyse wertet reCAPTCHA verschiedene Informationen aus (z. B. IP-Adresse, Verweildauer des Websitebesuchers auf der Website oder vom Nutzer getätigte Mausbewegungen). Die bei der Analyse erfassten Daten werden an Google weitergeleitet.\n\n\nDie reCAPTCHA-Analysen laufen vollständig im Hintergrund. Websitebesucher werden nicht darauf hingewiesen, dass eine Analyse stattfindet.\n\n\nDie Speicherung und Analyse der Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO. Der Websitebetreiber hat ein berechtigtes Interesse daran, seine Webangebote vor missbräuchlicher automatisierter Ausspähung und vor SPAM zu schützen. Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\n\n\nWeitere Informationen zu Google reCAPTCHA entnehmen Sie den Google-Datenschutzbestimmungen und den Google Nutzungsbedingungen unter folgenden Links: https://policies.google.com/privacy?hl=de und https://policies.google.com/terms?hl=de.\n\n\n\n\n\nImpressum vom Impressum Generator der Kanzlei Hasselbach, Rechtsanwälte für Arbeitsrecht und Familienrecht, sowie von https://www.e-recht24.de."
  },
  {
    "objectID": "blog/youtube-channel/index.html",
    "href": "blog/youtube-channel/index.html",
    "title": "I started a YouTube channel!",
    "section": "",
    "text": "This is a short blog post to share an exciting announcement with you. This thought has been on my mind for a while, and I finally decided to take the plunge."
  },
  {
    "objectID": "blog/youtube-channel/index.html#im-starting-a-youtube-channel",
    "href": "blog/youtube-channel/index.html#im-starting-a-youtube-channel",
    "title": "I started a YouTube channel!",
    "section": "✨ I’m starting a YouTube channel ✨",
    "text": "✨ I’m starting a YouTube channel ✨\nThe videos on the YouTube channel will fall into three categories:\n\nEarly career research: tips and advice based on my own experience\nTechnical topics: Data science, machine learning, and deep learning\nApplied programming: Visualization, reports, websites, and software design in R and Python\n\n\n\n;document.getElementById(\"tweet-51988\").innerHTML = tweet[\"html\"];"
  },
  {
    "objectID": "blog/youtube-channel/index.html#about-the-channel",
    "href": "blog/youtube-channel/index.html#about-the-channel",
    "title": "I started a YouTube channel!",
    "section": "About the channel 🧭",
    "text": "About the channel 🧭\nThe direction of the channel is not set in stone, and I’ll try to figure things out along the way. For now, my main goal is to get better at talking to the camera and practice video editing. If that piques your interest, I would be thrilled if you checked out my channel. In my first video, I talk about my background and my plans for the channel (i.e., I have none lol)"
  },
  {
    "objectID": "blog/youtube-channel/index.html#restructuring-my-website-and-newsletter",
    "href": "blog/youtube-channel/index.html#restructuring-my-website-and-newsletter",
    "title": "I started a YouTube channel!",
    "section": "Restructuring my website and newsletter 🏗",
    "text": "Restructuring my website and newsletter 🏗\nThe launch of the YouTube channel will lead to some restructuring work on my website overall. Setting up the channel made me think about what my vision is for future content. The blog will follow the same structure as the channel, with posts on early career research, technical ML/DL topics, and applied programming. I plan to use the tag system to make it easier for you to find the content you are interested in. This whole process may take a while and I appreciate your patience. For now, the newsletter (see bottom of the page) will mainly be used to send notifications about new content. I would like to focus on a consistent content schedule before I start sending out newsletters with additional content. So if you want to stay up to date and get occasional updates directly into your inbox, feel free to subscribe below (it’s free!)."
  },
  {
    "objectID": "blog/youtube-channel/index.html#what-are-your-thoughts",
    "href": "blog/youtube-channel/index.html#what-are-your-thoughts",
    "title": "I started a YouTube channel!",
    "section": "What are your thoughts? 🧠",
    "text": "What are your thoughts? 🧠\nYou would make my day if you took a minute to engage with my content. I truly appreciate any kind of feedback (e.g., my voice is too bass-heavy in the video) or comments. For example, you might want to let me know:\n\nWhat topics should I cover?\nDo you have positive examples of similar content that you enjoy?\nWhat mistakes should I avoid? What annoys you about other creators?\nWhat makes a video worth watching for you?\nHow can I add to the existing conversation on early career research and machine learning?"
  },
  {
    "objectID": "blog/youtube-channel/index.html#get-in-touch",
    "href": "blog/youtube-channel/index.html#get-in-touch",
    "title": "I started a YouTube channel!",
    "section": "Get in touch 📬",
    "text": "Get in touch 📬\nFeel free to drop me a message via email (mail.marvinschmitt@gmail.com), or reach out on any social media platform.\nThanks for reading, see you!\n–Marvin"
  },
  {
    "objectID": "blog/sbi-taxonomy/index.html#simulator",
    "href": "blog/sbi-taxonomy/index.html#simulator",
    "title": "Taxonomy of forward simulations",
    "section": "Simulator",
    "text": "Simulator\nThe simulator takes a parameter \\(\\theta\\), does its simulator thing, and outputs data \\(y\\). What happens inside the simulator is between you and god the mechanistic model that you’re coding up. For our simple Gaussian model, the parameter \\(\\theta\\) is the location (i.e., mean) of the distribution. The covariance is fixed to \\(1\\) to keep things simple.\nImportant: The distribution is 2-dimensional, so a single \\(\\theta\\) also has two dimensions. The first component of \\(\\theta\\) is the location on the \\(x\\)-axis, the second component of \\(\\theta\\) determines the location on the \\(y\\)-axis.\n\ntheta = np.array([[-1.0, 1.0]])\nprint(theta.shape)\n\n(1, 2)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: I initialize theta as (1, 2) rather than a flat (2, ). Later on, that’s gonna be useful when we want to simulate a batch of data sets at once.\n\n\n\n\n\n\n\n\n\n\n\nThis \\(\\theta=(-1, 1)\\) will be the location (i.e., mean) of our 2D normal distribution. Now we just have to simulate some data around this location. It’s up to us to choose how many data points we want to generate. Inspired by the empirical sciences, this very number of data points is called the number of observations or num_observations in our code. We default this to 50 to keep the function calls tidy, but we could modify that at any time.\n\ndef simulator(theta, num_observations = 50):\n    distribution = tfd.MultivariateNormalDiag(loc=theta, scale_diag=[1., 1.])\n    y = distribution.sample([num_observations])\n    y = tf.transpose(y, [1, 0, 2]) # change to num_datasets, num_observations, D\n    return y.numpy()\n  \ny = simulator(theta)\n\nThat’s our simulator with a straightforward API:\n\nInput: Parameter \\(\\theta\\in\\mathbb{R}^{\\text{num\\_datasets}\\times K}\\), where \\(K\\) is the parameter dimension (\\(K=2\\) in our case because we need a 2-component location information)\nOutput: Data \\(y\\in\\mathbb{R}^{\\text{num\\_datasets}\\times \\text{num\\_observations}\\times D}\\), where \\(D\\) is the data dimension (\\(D=2\\) in our case because we have a 2-dimensional Gaussian).\n\n\n\n\n\n\n\nNote\n\n\n\nIn many papers, this simulator is denoted as \\[y\\sim\\mathcal{N}(\\theta, \\boldsymbol{1}),\\] where both the number of observations and the data/parameter dimensions are implicit here. An alternative variant is the formulation on the observation-level: \\[y_m \\sim \\mathcal{N}(\\theta, \\boldsymbol{1})\\;\\;\\text{for}\\;m=1,\\ldots,M\\] 👆 In this notation, each single data point (one of the orange dots in the plot below) is denoted as \\(y_m\\) and the total number of observations num_observations is abbreviated as \\(M\\).\nYet another variant makes the dimensionality of the Gaussian explicit with a subscript such as \\(\\mathcal{N}_2\\) for our 2-dimensional Gaussian.\n\n\nLet’s check that all shapes match in our code:\n\ntheta = np.array([[-1.0, 1.0]])\ny = simulator(theta)\n\nprint(theta.shape) # should be (1, 2)\nprint(y.shape) # should be (1, 50, 2)\n\n(1, 2)\n(1, 50, 2)\n\n\nHere’s a visualization for the data \\(y\\) (orange), relative to the location parameter \\(\\theta\\) (blue) of the data-generating Gaussian distribution:\n\n\n\n\n\n\n\n\n\n\nBatch simulation\n\ntheta = np.array([[-2.5, 1.0], [2.0, -2.0], [2.0, 2.0]])\ny = simulator(theta)\nprint(y.shape)\n\n(3, 50, 2)\n\n\nNow things are getting interesting for batch-scale simulations! We first specified three parameter vectors, namely\n\n\\(\\theta^{(1)} = (-2.5, 1.0)\\),\n\\(\\theta^{(2)} = (2.0, -2.0)\\), and\n\\(\\theta^{(3)} = (2.0, 2.0)\\).\n\nThen, we plug this (3, 2) shaped parameter matrix into the simulator. Internally, the simulator slices the parameters along the first axis and generates a data set for each slice. This means that the first data set will have the location parameter \\(\\theta^{(1)} = (-2.5, 1.0)\\), or formally:\n\\[\ny^{(1)} \\sim \\mathcal{N}(\\theta^{(1)}, \\boldsymbol{1})\n\\]\nSimilarly, the second data set will be based on the second slice, and so on. Viewed as a single-step process, we create a batch of parameters and simultaneously generate a batch of data. This connection between parameter slice and generated data is obvious once we visualize it:\n\n\n\n\n\n\n\n\n\nWe end this section on batch simulation with a few notes on notation. Notation is a powerful tool, but it always bears the danger to become overly verbose and jeopardize the clarity of your work. Here, I’m not trying to use convoluted overloaded notation for the sake of it. Instead, I’m just trying to show you that this heavy notation is really just a systematic way to express how we slice stuff up and batch it in programming languages.\n\n\nNotation\nRecall that we generated 3 data sets \\(y^{(1)}, y^{(2)}, y^{(3)}\\). This can be expressed with the shorthand set notation \\(\\{y^{(i)}\\}_{i=1}^N\\) where \\(N=3\\) is the total number of data sets. So this is basically a set of data sets, and it is indexed by \\(i=1,\\ldots, N\\).\nFurther, each data set \\(y{(i)}\\) has \\(M=50\\) observations. We could, for instance, write \\[\ny^{(1)} = \\{y^{(1)}_1, y^{(1)}_2, y^{(1)}_3, \\ldots, y^{(1)}_{50}\\}\n\\] to refer to the single observations in the first data set \\(y^{(1)}\\). For exampe, the seventh observation (one small dot in the plots) of the second data set is \\(y^{(2)}_7\\).\nIf we ignore the inconvenience of 0-based (Python) vs. 1-based (math) indexing for a moment, the code access does just what our mathy indices do. Recall that the data y have shape (num_datasets, num_observations, D) and the parameters have shape (num_datasets, K). Similarly, we access the first data set \\(y^{(1)}\\) and the first parameter vector \\(\\theta^{(1)}\\) as:\n\nfirst_dataset = y[0, :, :] # first data set, all observations, all data dimensions\nfirst_parameter = theta[0, :] # first data set, all parameter dimensions\n\nplt.scatter(first_dataset[:, 0], first_dataset[:, 1])\nplt.scatter(first_parameter[0], first_parameter[1])\n\n\n\n\n\n\n\n\n\n\nThat’s enough notation for now. Back to modeling!"
  },
  {
    "objectID": "blog/sbi-taxonomy/index.html#prior",
    "href": "blog/sbi-taxonomy/index.html#prior",
    "title": "Taxonomy of forward simulations",
    "section": "Prior",
    "text": "Prior\nIf we’re interested in a probabilistic model, we can treat the mechanistic model parameter \\(\\theta\\) as a random variable itself. So now, \\(\\theta\\) is not restricted to \\((-1, 1)\\) as above. Instead, we place a suitable distribution \\(p(\\theta)\\) over the parameter. That’s called a prior distribution and it’s just a way of saying “these values for \\(\\theta\\) are reasonable for the mechanistic model”. This really sounds more complicated than it is. Let’s just use a uniform distribution for simplicity here. What about \\([-2, 0]\\) in \\(x\\)-direction and \\([-1, 1]\\) in \\(y\\)-direction?\n\ndef prior(num_simulations=1):\n    distribution = tfd.Independent(tfd.Uniform(\n      low=[ -3.0, -1.0], # both lower limits\n      high=[ 1.0,  2.0]), # both upper limits\n      reinterpreted_batch_ndims=1) \n    theta = distribution.sample([num_simulations])\n    return theta.numpy()\n\n\ntheta = prior(20)\n\nThis batched sampling statement returns 20 two-dimensional parameter vectors. Notationally, we can refer to those as \\(\\theta^{(1), \\theta^{(2), \\ldots, \\theta^{(100)}\\), or again in abbreviated set notation as \\(\\{\\theta^{(i)}\\}_{i=1}^N\\) where \\(N=20\\) is the total number of desired data sets. In the next section, I will bring it all together and explain why this is also the number of data sets even though those are just the parameters. Here’s a visualization of these 20 parameter vectors in 2D space. See how the sampled parameter values all lie nice and tight within our uniform bounding box that we defined in the prior distribution 🤩:"
  },
  {
    "objectID": "blog/sbi-taxonomy/index.html#data-generation",
    "href": "blog/sbi-taxonomy/index.html#data-generation",
    "title": "Taxonomy of forward simulations",
    "section": "Data generation",
    "text": "Data generation\nTo sum up, we have the following components for a model with data dimension \\(D\\) and parameter dimension \\(K\\):\n\nA simulator(theta, num_observations) function \\(f:\\theta \\mapsto y\\) with input \\(\\theta\\in\\mathbb{R}^{\\text{num\\_datasets}\\times K}\\) and output \\(y\\in\\mathbb{R}^{\\text{num\\_datasets}\\times\\text{num\\_observations}\\times D}\\).\nA prior(num_datasets) function with argument num_datasets (but no random variable as input) and output \\(\\theta\\in\\mathbb{R}^{\\text{num\\_datasets}\\times K}\\).\n\nSo basically, we can build a simple pipe simulator(prior(num_datasets)) because the simulator can infer num_datasets from \\(\\theta\\). After all, we choose to fix num_observations for convenience, and we naturally fix the data dimension \\(D\\) as well as the parameter dimension \\(K\\) when we define the model. That means the only argument that remains to specify is the number of data sets.\n\ndef generative_model(num_simulations):\n  theta = prior(num_simulations)\n  y = simulator(theta)\n  return theta, y\n\nThis pipe is called a generative model or forward model because it combines the prior \\(p(\\theta)\\) with the likelihood \\(p(y\\mid\\theta)\\). Mathematically, it implements the joint model:\n\\[\n\\underbrace{p(\\theta, y)}_{\\text{joint}} = \\underbrace{p(\\theta)}_{\\text{prior}}\\,\\underbrace{p(y\\mid\\theta}_{\\text{likelihood}})\n\\] and that’s exactly how we call it with the simple API we just threw together:\n\ntheta, y = generative_model(10)\n\nFor each of the 10 parameter vectors \\(\\theta{(i)}\\), we get one data set \\(y^{(i)}\\). Each cell of the following plot shows the \\(i^{th}\\) parameter vector (large dot, solid blue) and the corresponding \\(i^{th}\\) data set it generated (small dots, sky blue).\n\n\n\n\n\n\n\n\n\n\nNotation\n\\[(\\theta, y) \\sim p(\\theta, y)\\]\nOne draw from the generative model (i.e., one execution of generative_model(1)) returns the tuple \\(\\Big(\\theta^{(i)}, y^{(i)}\\Big)\\equiv\\Big(\\theta^{(i)}, \\{y^{(i)}_1, \\ldots, y^{(i)}_{M}\\} \\Big) \\equiv \\Big(\\theta^{(i)}, \\{y^{(i)}_m\\}_{m=1}^M \\Big)\\).\nBehind the scenes, the joint model is factorized into the prior and likelihood, and the step-wise sampling process with detailed notation would be:\n\\[\n\\begin{aligned}\n\\theta^{(i)} &\\sim p(\\theta) \\\\\ny^{(i)}_m &\\sim p(y\\mid\\theta^{(i)})\\quad \\text{for}\\;m=1,\\ldots,M\n\\end{aligned}\n\\]\n\n\n\n\n\n\nLearning from the joint model \\(p(\\theta, y)\\)\n\n\n\nIf you want to learn this forward problem (or the inverse problem; see below) from the joint model simulator, you’ll usually try to learn a map between one such parameter draw \\(\\theta^{(i)}\\) and the corresponding data set \\(y^{(i)}\\equiv\\{y^{(i)}_m\\}_{m=1}^M\\).\n\n\n\n\nSimulation budget\nWhen we say that some approximate algorithm \\(q_{\\phi}(\\cdot)\\) with learnable weights \\(\\phi\\) is being trained on a simulation budget of, say, \\(N=1\\,024\\), this means that all the approximate algorithm ever sees as examples from the joint model are \\(N=1\\,024\\) tuples of parameters and data.\nIn other words, image that you only get this single call to the generative model:\n\nN = 1024\ntheta, y = generative_model(N)\nprint(f\"parameter shape: {theta.shape}\")\nprint(f\"data shape: {y.shape}\")\n\nparameter shape: (1024, 2)\ndata shape: (1024, 50, 2)\n\n\nSo you get \\(N=1\\,024\\) ‘ground-truth’ pairs of data and parameters to work with, and that’s all."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Marvin.log()",
    "section": "",
    "text": "Welcome to my blog, which I called Marvin.log(). Here’s a loose collection of articles. Currently, there is no real ordering or coherent theme, but that might change in the future. I’m not very consistent with writing regular blog posts either – I’m proud that I made it past the lonely “Hello World!” post though. If you want to get a notification when I add a new blog post, you can enter your email address at the bottom of the page. It’s free and I won’t send spam. I appreciate any kind of feedback and I’m genuinely thankful for your interest and time."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Marvin.log()",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            October 15, 2024\n        \n        \n            Structured Language Generation with Outlines in R\n            \n            \n                \n                \n                    tech\n                \n                \n                \n                    programming\n                \n                \n            \n            \n            Calling the Outlines Python package in R with the reticulate package, with OpenAI's GPT-4o as a language model.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 10, 2024\n        \n        \n            Make your built-in laptop mic sound good in OBS\n            \n            \n                \n                \n                    technology\n                \n                \n            \n            \n            Some quick tips to improve the audio quality of your recordings and screen captures.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            July 12, 2024\n        \n        \n            I started a YouTube channel!\n            \n            \n                \n                \n                    announcement\n                \n                \n            \n            \n            The videos will be about early career research, technical ML/DL topics, and applied programming in R and Python. Check out my first video!\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            April 15, 2024\n        \n        \n            Download EMNIST manually\n            \n            \n                \n                \n                    programming\n                \n                \n                \n                    python\n                \n                \n                \n                    machine learning\n                \n                \n                \n                    technical\n                \n                \n            \n            \n            EMNIST is a classic image data set for machine learning. Sometimes the automatic PyTorch download fails, that bugs me. Here's a quick guide to download the EMNIST data set manually and make it work with PyTorch.\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Marvin.log()",
    "section": "2023",
    "text": "2023\n\n\n    \n    \n                  \n            July 10, 2023\n        \n        \n            Functional Programming in R, Part 1: Basics\n            \n            \n                \n                \n                    programming\n                \n                \n                \n                    functional programming\n                \n                \n                \n                    r\n                \n                \n                \n                    technical\n                \n                \n            \n            \n            Learn how to use functional programming in R to write more efficient, maintainable, and elegant code. This blog post covers the basics of functional programming, from passing functions as arguments to creating reusable, interconnected functions. Perfect for data scientists and R programmers looking to level up their coding skills and expand their data science toolkit.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            June 21, 2023\n        \n        \n            Every Scientist Should Have a Website\n            \n            \n                \n                \n                    website\n                \n                \n                \n                    academia\n                \n                \n            \n            \n            How putting yourself out there helps you show your research, expand your network, control your own content, reach an audience, and enhance your opportunities.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 28, 2023\n        \n        \n            Create Your Website with Quarto: Complete Tutorial and Template\n            \n            \n                \n                \n                    tech\n                \n                \n                \n                    website\n                \n                \n                \n                    academia\n                \n                \n                \n                    quarto\n                \n                \n            \n            \n            This tutorial and template will help you build your own personal website with Quarto. It is a full guide with step-by-step instructions for your personal or academic homepage.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 7, 2023\n        \n        \n            Agile methods in academia\n            \n            \n                \n                \n                    psychology\n                \n                \n                \n                    academia\n                \n                \n            \n            \n            Boost your productivity and feel less overwhelmed with these six agile methods.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            February 3, 2023\n        \n        \n            Pre-releasing the {ggsimplex} R package\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    R\n                \n                \n                \n                    programming\n                \n                \n                \n                    visualization\n                \n                \n            \n            \n            Here's a quick rundown of this ggplot extension for point plots and density plots in a (2-)simplex. These can be used to visualize compartmental data, posterior model probabilities, or densities with support on a probability simplex.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 24, 2023\n        \n        \n            Quick hack: Mac email shortcut\n            \n            \n                \n                \n                    tech\n                \n                \n            \n            \n            Create a custom keyboard shortcut for your email addresses on Mac.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            January 7, 2023\n        \n        \n            4 Realizations from Atomic Habits\n            \n            \n                \n                \n                    psychology\n                \n                \n                \n                    books\n                \n                \n            \n            \n            Discover the Power of Atomic Habits: A Scientific Approach to Building Good Habits and Breaking Bad Ones. Let me explain ...\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Marvin.log()",
    "section": "2022",
    "text": "2022\n\n\n    \n    \n                  \n            May 20, 2022\n        \n        \n            Hello World!\n            \n            \n                \n                \n                    news\n                \n                \n            \n            \n            For the past years, my homepage has just been a placeholder. It served a single purpose: People who enter the domain of my email address in their browser should not look at some weird empty page. So here we go!\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items\n\n\n\nThe blog post listing is based on the website source of Andrew Heiss, who has put together an incredible listing template under CC-BY-SA 4.0 license. Thank you!"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html",
    "href": "blog/website-tutorial-quarto/index.html",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "",
    "text": "Quick Links:     Download the free Template     Live preview of the template"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#my-template-for-you",
    "href": "blog/website-tutorial-quarto/index.html#my-template-for-you",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.1 My template for you",
    "text": "2.1 My template for you\nYou do not have to start the project of creating your own personal website from scratch because I have crafted a template for you. You can take the template, fill in your information, and leave it as-is. Alternatively, if you find that crafting your own website sparks joy for you, and you want to further delve into the nuts and bolts, you can simply take this template as a basis and extend your website from there.\nYou can take a look at the live preview here. You find the free direct download link for the entire template here.\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\nThe first step is to download the website template. You can either clone the GitHub repository or download a .zip archive with the template here:\n\nDownload the template"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#opening-the-template-in-rstudio",
    "href": "blog/website-tutorial-quarto/index.html#opening-the-template-in-rstudio",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.2 Opening the template in RStudio",
    "text": "2.2 Opening the template in RStudio\nUnpack the archive and open the quarto-website-template.Rproj file in RStudio. If R and RStudio are not installed on your machine, check out this link. You should see a new Build tab in the upper-right panel of RStudio with a button Render Website:\n\n\n\nIf you open the template project, Rstudio should show a Build tab containing a Render Website button\n\n\nIf you do not see that tab and button, please use the most powerful debugging tool known to mankind: Restart RStudio."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#global-configuration",
    "href": "blog/website-tutorial-quarto/index.html#global-configuration",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.3 Global configuration",
    "text": "2.3 Global configuration\nOpen the file _quarto.yml in the root directory. This is where you control the settings of your website. The YAML format is a compromise between a machine-readable and a human-readable file. The good news is: You don’t have to write new YAML blocks; you just have to enter some info in the appropriate places. Let’s go through the _quarto.yml file step by step.\n\n2.3.1 Author info\nIt’s time to tell the readers about yourself. Fill in your details, such as your name (title field), a brief description, and a global image of yourself.\n\n\n2.3.2 Navigation\nThe navigation in the template is split in two parts. The left side contains icons for your social links, such as Twitter, GitHub, or email.\nThe right side links to other pages of your website. Each navigation entry consists of a title (text), followed by a link (href). Take a look at the example entries and continue the pattern as you see fit.\n\n\n2.3.3 Advanced settings\nThe rest of the _quarto.yml file just controls that we will indeed create a website, our preview tab doesn’t change all the time, and we can publish our website without complications. Don’t mess with this part unless you know what you are doing. If you do know what you are doing: Move fast and break things 😉\n\n\n2.3.4 More config tweaks\nThis blog post covers the fundamentals to set you up with a working website. Are you hungry for more and want to pull an all-nighter because creating a website can be super addictive? The official Quarto Documentation contains a bunch of other settings you can tweak via the _quarto.yml configuration. Happy hacking, and let me know if you find something you want me to cover in a future blog post!"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#the-landing-page",
    "href": "blog/website-tutorial-quarto/index.html#the-landing-page",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.4 The landing page",
    "text": "2.4 The landing page\nIn the root directory of the template folder, you find the file index.qmd. This is the landing page of your website. This is the first impression that visitors have from your website. It is a great opportunity to tell them a bit about yourself and spark their interest.\nIn this template, we are not designing a full landing page from scratch. Instead, we use one of the great template designs that are shipped with Quarto. Template inception. We will basically feed Quarto the info to create a pretty “About” section, where users see your image, some social links, and a brief description.\nOpen the /index.qmd file and inspect the header, that is, the part at the top of the source code which is enclosed in --- delimiters:\n\n\n/index.qmd\n\n---\nabout:\n  template: jolla\n  id: about-block\n  image: img/my_image.png\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/MarvinSchmittML\n    - icon: github\n      text: Github\n      href: https://github.com/marvinschmitt\n    - icon: linkedin\n      text: LinkedIn\n      href: https://www.linkedin.com/in/marvin-schmitt-a85b321a2/\n    - icon: envelope\n      text: Email\n      href: \"mailto:mail.marvinschmitt@gmail.com\"  \n---\n\nAll following fields are nested in the about field, which is our way of telling Quarto that all the subsequent information should be associated with our “About” section. The template is called jolla, and you can find other options here. The id field assigns a name to the “About” section such that you can insert it anywhere in your page. As the name might suggest, the image field contains the path of an image to display on your landing page. Finally, the links field contains a bunch of social links. Each social entry consists of three fields:\n\nicon: identifier of an icon, such as twitter or envelope. You can take a look at the Standard Bootstrap5 Icons for more icon names.\ntext: description of the entry, usually for accessibility\nhref: link, make sure to add mailto: before an email address\n\nAs you can see in the /index.qmd file, the “About” block is inserted in the actual document via\n\n\n/index.qmd\n\n::: {#about-block}\n:::\n\nand you can add a nice introduction about yourself below the block."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#customizing-the-colors",
    "href": "blog/website-tutorial-quarto/index.html#customizing-the-colors",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.5 Customizing the colors",
    "text": "2.5 Customizing the colors\nI have prepared a style file at html/styles.scss, which contains the core configuration from the fabulous website of Andrew Heiss. This section defines the colors of the website which are used throughout many pages.\n\n\nhtml/styles.scss\n\n$primary:   $teal!default;\n$secondary: $gray-700 !default;\n$success:   $green !default;\n$info:      $cyan !default;\n$warning:   $orange !default;\n$danger:    $red !default;\n$light:     $gray-400 !default;\n$dark:      $black !default;\n\nThe most important field is arguably $primary. If you want your website to look mainly green, just change that line to $primary: $green!default;. Note that $teal and $green are SCSS variables, which are defined further up in the file. If you want to fine-tune your color palette, you can change these HEX codes."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#adding-new-pages",
    "href": "blog/website-tutorial-quarto/index.html#adding-new-pages",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.6 Adding new pages",
    "text": "2.6 Adding new pages\nIf you want to add a new page with the name mypage to your website, create a folder mypage/ and add a file index.qmd in that folder. This has one subtle advantage over creating mypage.qmd in the base directory. If you created /mypage.qmd, the URL would be www.yourname.com/mypage.html. That exposed .html in the URL looks very 2010. Instead, you should go with /mypage/index.qmd and get the modern-looking URL www.yourname.com/mypage.\n\n\n\nAvoid .html in your URL by using a subfolder with index.html.\n\n\nOnce you have created your brand-new index.qmd, just copy the code skeleton of any other index.qmd from the template and start adding your content. If you wish to, add the newly created page to the navigation (see Section 2.3.2). That’s it, you have successfully extended your website 🚀"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#setting-up-your-github-account",
    "href": "blog/website-tutorial-quarto/index.html#setting-up-your-github-account",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.1 Setting up your GitHub account",
    "text": "3.1 Setting up your GitHub account\nFirst, you create a free GitHub account at www.github.com. This is the go-to place for many programmers from various fields for all things around developing software. GitHub is an invaluable resource if you want to collaborate with others or simply build a portfolio of your own work to strengthen your next job application. Even the comments section of this blog is powered by GitHub (as of March 2023).\nEven if you come to the conclusion that you don’t actually want to build a personal website: If you are writing code in any way, please create a GitHub account now if you don’t have one already 😉\nNote: If you deploy your website through GitHub pages, your website will live at &lt;username&gt;.github.io – so you might want to avoid usernames like N00bSlayer420. n00bslayer420.github.io might not look as cool on your resume, just saying."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#preparing-your-website-for-github-pages",
    "href": "blog/website-tutorial-quarto/index.html#preparing-your-website-for-github-pages",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.2 Preparing your website for GitHub pages",
    "text": "3.2 Preparing your website for GitHub pages\nThe template already contains all necessary settings for GitHub pages, so you don’t have to do anything in this section. However, it’s always good to have a superficial understanding of how things work, and it’s fairly staightforward in this case. We use the simplest form of GitHub pages: We build the entire website locally (i.e., in RStudio) and output the built page into the docs/ folder. This is configured in the output-dir field of the almighty _quarto.yml:\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\nWe commit this docs/ folder to GitHub, and GitHub pages simply uses that folder to serve our website. This has one important consequence: If we make any change to our website, we have to Render the website again in RStudio. That’s how the updates make it to the docs/ folder! Only changing the .qmd file is not enough because GitHub does not render our site directly from the .qmd source.\nIf you are not familiar with programming at all, just imagine it like this: You are a real estate agent with a downtown office. You have your current listings in your street-facing window. You always write your listings in Microsoft Word, and those documents are of course saved as .docx files on your computer. However, those documents somehow have to get from your computer to the window display.\nThat’s the job of your friendly assistant Paige. You and Paige have agreed on the following workflow. You have a folder docs/ where you save all current listings as PDF files. Paige will monitor this docs/ folder, and whenever there is a change, Paige will print the PDF and replace the listing in the window display. Sounds like a straightforward process, right? That’s pretty much what we are doing in our deployment pipeline as well:\n\n\n\n\nReal Estate Agent\nYou\n\n\n\n\nEnvironment\nMicrosoft Word\nRStudio\n\n\nSource files\n.docx\n.qmd\n\n\nOutput folder\ndocs/\ndocs/\n\n\nDeployment\nAssistant (Paige)\nGitHub Pages\n\n\nOutput format\n.pdf\n.html\n\n\nServing platform\nWindow display\n&lt;domain&gt;.github.io\n\n\n\nNote to advanced readers: You can set up a GitHub action to let GitHub build the website directly from the source files. But that deserves its own tutorial since some bugs persist which make things awkward (as of February 2023). If you would like to see automatic deployment covered in a future blog post, please let me know!\nThe second change to our website files is to include an empty file with the name .nojekyll (already included in the template). Usually, GitHub pages does some post-processing on your website files with Jekyll. Since Quarto handles all the processing we need, we want to deactivate Jekyll’s post-processing. Adding a .nojekyll file to your website root directory does exactly that: It bypasses post-processing via Jekyll."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#deployment",
    "href": "blog/website-tutorial-quarto/index.html#deployment",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.3 Deployment 🥳",
    "text": "3.3 Deployment 🥳\nWe are almost there!\nYou will now create a new repository with the name &lt;github_account_name&gt;.github.io, where &lt;github_account_name&gt; is the exact name of your GitHub account. It is important that you follow this pattern, because &lt;github_account_name&gt;.github.io is the only repository that will be hosted directly to the URL &lt;github_account_name&gt;.github.io.\nIn my case, my GitHub name is marvinschmitt, so my website repository would be called marvinschmitt.github.io and the URL of my website will be marvinschmitt.github.io.\nNavigate to the newly created repository, select Add files via Upload, and add the entire folder content of our RStudio website project. Your repository should be populated now and look somewhat similar to this:\n\n\n\nThis is what the repository should look like after you upload data.\n\n\nOn the GitHub repository, go to Settings and click on GitHub Pages. Make sure that GitHub pages will deploy from a branch (1), the branch is set to main or master (2), and GitHub Pages is served from the docs/ folder (3):\n\n\n\nGitHub pages deploys from a branch (1), the branch is set to main or master (2), and it serves from the docs/ folder\n\n\nGo back to the repository and you’ll see a brown dot indicating that a build is in progress. This can take a couple of minutes. Once the deploy is done, the indicator dot will turn green. That’s it. Congratulations, your website is now live at &lt;yourname&gt;.github.io! 🎈🥂🥳"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#optional-managing-github-directly-through-git",
    "href": "blog/website-tutorial-quarto/index.html#optional-managing-github-directly-through-git",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.4 (Optional) Managing GitHub directly through git",
    "text": "3.4 (Optional) Managing GitHub directly through git\nManually uploading your files to GitHub after every update can feel tedious.\n\n3.4.1 Option 1: Connect GitHub and RStudio\nThis great tutorial shows how you can directly connect GitHub and RStudio with a few simple steps. This makes uploading updates to your website as easy as pushing “Commit” and “Push” in RStudio.\n\n\n3.4.2 Option 2: GitHub Desktop\nIf you don’t want to manage the GitHub repository directly in Rstudio but don’t want to use the command line either, you can use GitHub Desktop as an in-between solution. I can highly recommend GitHub Desktop, it’s really easy to set up and use, even if you don’t have any experience with git."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#general-web-development-tips",
    "href": "blog/website-tutorial-quarto/index.html#general-web-development-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.1 General web development tips",
    "text": "5.1 General web development tips\nHere are some general web development tips, ranging from file management and DNS over to HTML, CSS, and JavaScript.\n\n5.1.1 Relative links\n\nThanks to Paul Buerkner for requesting this\n\nSuppose you are editing the file projects/index.qmd and want to add a link to the page &lt;name&gt;.github.io/photography. You can of course link to the full URL &lt;name&gt;.github.io/photography, but that link will break if you change your domain name to www.&lt;name&gt;.com. Instead, you can use relative links. You only need to know that ../ takes you one layer back on the folder tree.\nExample: Starting from projects/index.qmd, you can use ../photography/index.qmd to walk back one layer into the root folder of your website, and then move into the folder photography/, where index.qmd of photography lives. This relative path is valid no matter what domain you use. It only breaks if you decide to remodel the entire structure of your website. But in this case, I assume you know what you are doing and you’re well off on your own 😉\n\n\n5.1.2 CSS flavors\n\nThanks to Andrew Heiss for making me aware of SCSS from studying the source code of his website\n\nQuarto supports some cool CSS variants, such as SCSS. In SCSS, you can add variables such as $orange to define your favorite color once and use it in many different places. This deserves a separate blog post, though.\n\n\n5.1.3 Include Order\nIf you mix HTML with CSS and JavaScript, make sure to include the files in the correct order. If your JavaScript manipulates existing HTML objects through the DOM, make sure to include the JavaScript file after the HTML body. If you include the JavaScript before the HTML body, JavaScript will not know that the HTML objects exist, and you will have a bad time. Been there, spent well over 2 hours debugging. 0/10, cannot recommend."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#quarto-specific-tips",
    "href": "blog/website-tutorial-quarto/index.html#quarto-specific-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.2 Quarto specific tips",
    "text": "5.2 Quarto specific tips\nHere are some tips that are specific to Quarto, Markdown, or RMarkdown.\n\n5.2.1 Adding a table of contents\n\nThanks to Paul Buerkner for requesting this\n\nThanks to Quarto, it’s super easy to add a table of contents. Just add the following lines to the header of your .qmd file, without any indentation:\n\n\nindex.qmd\n\ntoc: true\ntoc-title: Contents\ntoc-location: left\n\n\n\n5.2.2 Labels and references\nIf you are familiar with \\(\\LaTeX\\), you might appreciate proper references with \\label{sec:introduction} and \\autoref{sec:introduction}. The Quarto equivalent is {#sec-introduction} for the label and @sec-introduction for the reference."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#more-tips",
    "href": "blog/website-tutorial-quarto/index.html#more-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.3 More tips?",
    "text": "5.3 More tips?\nIf you know any other quick tips that you would like to see here, just leave a comment below or drop me a message. I will be happy to include it (and credit you, of course) 🚀"
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-1-breaking-down-a-project-into-smaller-tasks",
    "href": "blog/agile-methods-for-academia/index.html#method-1-breaking-down-a-project-into-smaller-tasks",
    "title": "Agile methods in academia",
    "section": "3.1 Method 1: Breaking down a project into smaller tasks",
    "text": "3.1 Method 1: Breaking down a project into smaller tasks\nBreaking down a large project into smaller, more manageable tasks is a common agile method that can help you focus on what needs to be done. This method is especially helpful if you tend to feel overwhelmed by the sheer scope of a project.\nFor instance, if you’re working on a research paper, you can break it down into smaller tasks such as conducting a literature review, drafting an outline, writing the introduction, collecting data, analyzing data, and writing the conclusion. This approach helps you manage your time more effectively and makes it easier to track your progress."
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-2-specifying-the-definition-of-done",
    "href": "blog/agile-methods-for-academia/index.html#method-2-specifying-the-definition-of-done",
    "title": "Agile methods in academia",
    "section": "3.2 Method 2: Specifying the “definition of done”",
    "text": "3.2 Method 2: Specifying the “definition of done”\nEach task should have a clear “definition of done”, which means identifying what needs to be done to complete that task. For example, if I am working on a plot for a research paper, my definition of done includes:\n\n✅ a colorblind-friendly palette,\n✅ a clear legend, proper labels,\n✅ a descriptive caption,\n✅ subfigures that match with alignment and font size.\n\nSpecifying the “definition of done” helps you set clear expectations and ensures that you’re meeting the requirements for each task. It also helps you avoid the feeling of being stuck in a never-ending cycle of revisions. If you’re done (according to your definition of done) with all tasks, you can still re-iterate and polish the entire project."
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-3-conducting-retrospectives",
    "href": "blog/agile-methods-for-academia/index.html#method-3-conducting-retrospectives",
    "title": "Agile methods in academia",
    "section": "3.3 Method 3: Conducting retrospectives",
    "text": "3.3 Method 3: Conducting retrospectives\nRetrospectives are crucial for reflecting on your workflow, identifying areas for improvement, and making changes for future projects. By reflecting on your successes and failures, you can work more efficiently and improve your research processes.\nTo perform a retrospective, the team should set aside a specific time to reflect on the completed project or a particular phase of the project. Each team member should be encouraged to share their thoughts, feelings, and feedback openly and honestly. The team should discuss what worked well, what didn’t work, and what could be improved. Retrospectives should be conducted regularly, preferably after each iteration, to continuously improve the workflow and achieve better results.\nAs a junior researcher, you may not have the opportunity to issue a retrospective with all members of a project. However, you can still have a solo retrospective on your tasks. You can ask yourself questions like “What worked well and should be repeated next time?” or “What didn’t work out well and should be changed right away?” or “What are the next steps, and when are they due?”"
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-4-setting-work-in-progress-limits",
    "href": "blog/agile-methods-for-academia/index.html#method-4-setting-work-in-progress-limits",
    "title": "Agile methods in academia",
    "section": "3.4 Method 4: Setting work-in-progress limits",
    "text": "3.4 Method 4: Setting work-in-progress limits\nSetting a limit on the number of tasks you’re working on can help you avoid feeling overwhelmed and stay focused on the task at hand. For instance, you can limit yourself to two to three tasks for maximum efficiency.\nBy setting work-in-progress limits, you can avoid overcommitting and ensure that you’re making progress on the right tasks.\nalso helps you prioritize your work and avoid the tendency to multitask, which can be counterproductive."
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-5-estimating-the-required-amount-of-work-for-each-task",
    "href": "blog/agile-methods-for-academia/index.html#method-5-estimating-the-required-amount-of-work-for-each-task",
    "title": "Agile methods in academia",
    "section": "3.5 Method 5: Estimating the required amount of work for each task",
    "text": "3.5 Method 5: Estimating the required amount of work for each task\nEstimating the required amount of work for each task helps you plan your time more effectively and avoid over-committing. It also facilitates communication with collaborators by setting realistic expectations for when they can expect results.\nEstimating the required amount of work for each task also helps you avoid underestimating the amount of work required for a task, which can lead to delays and missed deadlines."
  },
  {
    "objectID": "blog/agile-methods-for-academia/index.html#method-6-prioritizing-tasks",
    "href": "blog/agile-methods-for-academia/index.html#method-6-prioritizing-tasks",
    "title": "Agile methods in academia",
    "section": "3.6 Method 6: Prioritizing tasks",
    "text": "3.6 Method 6: Prioritizing tasks\nPrioritizing tasks based on their importance and urgency is an essential skill for managing research projects. The Eisenhower Matrix is a useful tool for prioritizing tasks based on four categories: urgent and important, important but not urgent, urgent but not important, and neither urgent nor important.\n\nIllustration of the Eisenhower matrix for prioritizing tasks.\n\n\n\nurgent\nnot urgent\n\n\n\n\nimportant\nDo it now.\nSchedule a time.\n\n\nnot important\nDelegate it.\nDelete it.\n\n\n\nBy using the Eisenhower Matrix, you can prioritize your tasks and make progress toward your goals without feeling overwhelmed. Learning to say no is also an essential skill in prioritizing tasks."
  },
  {
    "objectID": "blog/emnist-manual-loading/index.html#problem-automatic-emnist-download-failed",
    "href": "blog/emnist-manual-loading/index.html#problem-automatic-emnist-download-failed",
    "title": "Download EMNIST manually",
    "section": "Problem: Automatic EMNIST download failed",
    "text": "Problem: Automatic EMNIST download failed\nEarlier today, I wanted to reproduce the results of a machine learning paper that uses the EMNIST digits data set to train a PyTorch model. Normally, PyTorch makes loading and even downloading data sets extremely easy for us. The torchvision.datasets module provides a handful of commonly used data sets with a user-friendly API. Most importantly for us right now, the data set loaders come with the convenient download=True argument to download a data set automatically:\n\nimport torchvision\n\ntrain_data = torchvision.datasets.EMNIST(\n  root=\"./\", \n  split=\"digits\", \n  train=True,\n  download=True\n)\n\nUnfortunately, that throws a RuntimeError:\nRuntimeError: File not found or corrupted.\nNext, I wanted to just download the data from a URL via torchvision.datasets.util.download_url(...). I found a handful of EMNIST URLs on the internet, but either got the same old File not found or corrupted or an SSL error."
  },
  {
    "objectID": "blog/emnist-manual-loading/index.html#fix-manual-download-and-directory-adjustments",
    "href": "blog/emnist-manual-loading/index.html#fix-manual-download-and-directory-adjustments",
    "title": "Download EMNIST manually",
    "section": "Fix: Manual download and directory adjustments",
    "text": "Fix: Manual download and directory adjustments\nHere’s a brief list of steps for downloading the EMNIST data manually and then preparing the directory for torchvision.datasets.EMNIST(..., download=False).\n\nStep 1: Download the files\nGo to the official EMNIST website (Link) and head to Binary format as the original MNIST dataset. Alternatively, here’s the link: EMNIST Direct Download Link\nThat archive with the great name gzip.zip has a size of approximately 500MB.\n\n\nStep 2: Unpack the gzip.zip archive\nHead to your project’s data directory (or global data directory if you have that) and unpack the previously downloaded gzip.zip archive there. You will get a folder gzip/ that contains a whole lot of *.gz files:\n.\n└── gzip\n    ├── emnist-balanced-mapping.txt\n    ├── emnist-balanced-test-images-idx3-ubyte.gz\n    ├── emnist-balanced-test-labels-idx1-ubyte.gz\n    ├── emnist-balanced-train-images-idx3-ubyte.gz\n    ├── emnist-balanced-train-labels-idx1-ubyte.gz\n    ├── ...\n    ├── emnist-digits-mapping.txt\n    ├── emnist-digits-test-images-idx3-ubyte.gz\n    ├── emnist-digits-test-labels-idx1-ubyte.gz\n    ├── emnist-digits-train-images-idx3-ubyte.gz\n    ├── emnist-digits-train-labels-idx1-ubyte.gz\n    ├── ...\n    ├── emnist-mnist-mapping.txt\n    ├── emnist-mnist-test-images-idx3-ubyte.gz\n    ├── emnist-mnist-test-labels-idx1-ubyte.gz\n    ├── emnist-mnist-train-images-idx3-ubyte.gz\n    └── emnist-mnist-train-labels-idx1-ubyte.gz\n\n\n\n\n\n\nEMNIST splits\n\n\n\nYou’ll notice a structure: There are different splits, encoded in the filenames as emnist-&lt;split&gt;-.... This &lt;split&gt; corresponds to the split=... argument in torchvision.datasets.EMNIST. For this project, I only needed the digits split, so I deleted the files of all the other splits.\n\n\n\n\nStep 3: Unpack the individual .gz files\nUnpack all the *.gz files that you need. On MacOS, the built-in archive tools can handle .gz files, YMMV. Delete the *.gz files after you’re done unpacking. You should have the following structure now:\n.\n└── gzip\n    ├── emnist-balanced-mapping.txt\n    ├── emnist-balanced-test-images-idx3-ubyte\n    ├── emnist-balanced-test-labels-idx1-ubyte\n    ├── emnist-balanced-train-images-idx3-ubyte\n    ├── emnist-balanced-train-labels-idx1-ubyte\n    ├── ...\n    ├── emnist-digits-mapping.txt\n    ├── emnist-digits-test-images-idx3-ubyte\n    ├── emnist-digits-test-labels-idx1-ubyte\n    ├── emnist-digits-train-images-idx3-ubyte\n    ├── emnist-digits-train-labels-idx1-ubyte\n    ├── ...\n    ├── emnist-mnist-mapping.txt\n    ├── emnist-mnist-test-images-idx3-ubyte\n    ├── emnist-mnist-test-labels-idx1-ubyte\n    ├── emnist-mnist-train-images-idx3-ubyte\n    └── emnist-mnist-train-labels-idx1-ubyte\n\n\nStep 4: Adjust the directory structure for PyTorch\nIf we try to load the data set into PyTorch with the download=False argument now,\n\ntrain_data = torchvision.datasets.EMNIST(\n  root=\"./\", \n  split=\"digits\", \n  train=True,\n  download=False\n)\n\nwe get the following error:\nRuntimeError: Dataset not found. You can use download=True to download it\nWell, we kind of did all the downloading so that we circumvent the problematic download=True call.\nAs you might expect, we have to make PyTorch find our downloaded EMNIST data. That’s a two-step process: (1) We will make the EMNIST data fit the format that PyTorch expects; and (2) we will point PyTorch to where our EMNIST data lives.\n\n(1) Required directory tree\nPyTorch wants the following structure:\nDATASET_NAME\n└── raw\n    ├── ...-mapping.txt\n    ├── ...-ubyte\nTo achieve this, we simply rename gzip to raw and wrap the entire raw folder into a parent folder called EMNIST. Now your file tree should look like this:\nEMNIST\n└── raw\n    ├── emnist-balanced-mapping.txt\n    ├── emnist-balanced-test-images-idx3-ubyte\n    ├── emnist-balanced-test-labels-idx1-ubyte\n    ├── emnist-balanced-train-images-idx3-ubyte\n    ├── emnist-balanced-train-labels-idx1-ubyte\n    ├── ...\n    ├── emnist-digits-mapping.txt\n    ├── emnist-digits-test-images-idx3-ubyte\n    ├── emnist-digits-test-labels-idx1-ubyte\n    ├── emnist-digits-train-images-idx3-ubyte\n    ├── emnist-digits-train-labels-idx1-ubyte\n    ├── ...\n    ├── emnist-mnist-mapping.txt\n    ├── emnist-mnist-test-images-idx3-ubyte\n    ├── emnist-mnist-test-labels-idx1-ubyte\n    ├── emnist-mnist-train-images-idx3-ubyte\n    └── emnist-mnist-train-labels-idx1-ubyte\n\n\n(2) Point PyTorch to the correct path.\nFinally, the call to the PyTorch data loader will work as intended because the EMNIST folder is directly below my current working directory ./:\n\ndata_root = \"./\"\n\ntrain_data = torchvision.datasets.EMNIST(\n  root=data_root, \n  split=\"digits\", \n  train=True,\n  download=False\n)\n\nIf your EMNIST/ folder lives somewhere else (e.g., in a dedicated data/ folder), simply adjust data_root.\n\n\n\nStep 5: Profit!\nNow off you go and make some fancy machine learning stuff with EMNIST! ✨\n–Marvin"
  },
  {
    "objectID": "blog/mac-setup/index.html#settings",
    "href": "blog/mac-setup/index.html#settings",
    "title": "Get the most out of your Mac with this setup",
    "section": "Settings",
    "text": "Settings\n\nTrackpad: Tap to Click\nShow Battery Percentage\nRemove these Apps\n\nPages\nKeynote\nNumbers\n\nKeyboard Settings\n\nDon’t Adjust Keyboard Brightness in Low Light\nDisable all Autocorrect related Items\nSet Keyboard Shortcuts for @@x Mail\nDisable Spotlight Shortcut\n\nFinder:\n\nPreferences\n\nNew Finder Windows show home ~\nSidebar: No Tags, no iCloud\nAdvanced — Show Filename Extensions\nAdvanced — Keep Folders on Top\nAdvanced — Empty Bin after 30 days\n\nWindow View\n\nView → Show Path Bar\nView → Show Status Bar\nCommand + J → Sort By → Snap to Grid\n\nView Options\n\nSort by → Name (set as default except for ~/Downloads)"
  },
  {
    "objectID": "blog/mac-setup/index.html#dock-customization",
    "href": "blog/mac-setup/index.html#dock-customization",
    "title": "Get the most out of your Mac with this setup",
    "section": "Dock Customization",
    "text": "Dock Customization\nThe custom position of the Dock is at the bottom of the screen. That looks neat, but it feels like a waste of space to me. I prefer having the dock on either the left or the right side of my screen. Navigate to System Preferences -&gt; Dock & Menu Bar and switch Positon on screen to your preferred setting.\nGrouping similar apps in the Dock is handy to keep the Dock organized. Yet, an actual folder hides the apps and increases the time to access an app. That’s where a small separator would come in handy. Unfortunately, there is currently no built-in option to add separators in the graphical user interface. But we can use the Terminal to add a transparant spacer tile and group apps with that. Just open Terminal in the Applications and type in the following commands. The first line adds the spacer tile, and the second line restarts the Dock so you see the effect right away:\ndefaults write com.apple.dock persistent-apps -array-add '{\"tile-type\"=\"small-spacer-tile\";}';\nkillall Dock\n\nHiding the dock when it is not needed\nHead over to System Preferences -&gt; Dock & Menu Bar and turn on Automatically hide and show the Dock. Now, the Dock disappears when you don’t need it, so you reclaim a large portion of screen real estate when you are not actually using the Dock. But the animation feels sluggish. Let’s speed it up a bit!\nSo finally, we use the Terminal again to shorten the duration of the Dock animation. Saving a fraction of a second over and over adds up ;) The first line removes the delay from the hide animation. This means that the animation starts right when you move your mouse cursor away from the dock. The second line shortens the actual animation duration to 0.2 seconds. Again, the last line restarts the Dock so that you see the effect take place:\ndefaults write com.apple.dock autohide-delay -float 0; \ndefaults write com.apple.dock autohide-time-modifier -float 0.2;\nkillall Dock"
  },
  {
    "objectID": "blog/mac-setup/index.html#apps",
    "href": "blog/mac-setup/index.html#apps",
    "title": "Get the most out of your Mac with this setup",
    "section": "Apps",
    "text": "Apps\n\nMust haves\n\nAlfred 4\n\nSync Preferences\n\nNotion\nDiscord\nSpotify\nChrome\nTeX\n\nMacTeX\nTeXStudio\nremove unused programs that came with MacTeX\n\nR\n\nR Kernel\nRstudio\n\nGit\nOffice 365\n\nRemove unused programs\n\n\n\n\nNice to have, but specific for my setup\n\nAffinity\n\nPhoto\nDesigner\nPublisher\n\nOBS\nElgato Control Center\nZoom\nWebex\nLogitech Options"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "This page contains a brief overview of projects that I significantly shaped throughout the entire project life cycle. In academic terms, this mostly corresponds to first-author publications (single and shared). If you’re interested in a full list of projects I have been involved in, please check out my CV."
  },
  {
    "objectID": "projects/index.html#cmpe",
    "href": "projects/index.html#cmpe",
    "title": "Projects",
    "section": "Consistency Model Posterior Estimation",
    "text": "Consistency Model Posterior Estimation\n\nPaper (NeurIPS 2024)\nConsistency models for neural posterior estimation (CMPE) is a new conditional sampler for scalable, fast, and amortized simulation-based inference with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem."
  },
  {
    "objectID": "projects/index.html#multinpe",
    "href": "projects/index.html#multinpe",
    "title": "Projects",
    "section": "Deep Fusion for Multimodal Simulation-Based Inference",
    "text": "Deep Fusion for Multimodal Simulation-Based Inference\n\nPreprint (arXiv)\nWe present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy and better performance under partially missing data."
  },
  {
    "objectID": "projects/index.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "href": "projects/index.html#data-efficient-amortized-bayesian-inference-via-self-consistency-losses",
    "title": "Projects",
    "section": "Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses",
    "text": "Data-Efficient Amortized Bayesian Inference via Self-Consistency Losses\n\nFull Paper (ICML 2024) | Short Paper (NeurIPS UniReps 2023) | Poster (ICML 2024)\nWe propose a method to improve the efficiency and accuracy of amortized Bayesian inference by leveraging universal symmetries in the probabilistic joint model \\(p(\\theta,y)\\). In a nutshell, we invert Bayes’ theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators."
  },
  {
    "objectID": "projects/index.html#meta-uncertainty-BMC",
    "href": "projects/index.html#meta-uncertainty-BMC",
    "title": "Projects",
    "section": "Meta-Uncertainty in Bayesian Model Comparison",
    "text": "Meta-Uncertainty in Bayesian Model Comparison\n\nPaper (AISTATS 2023) | Code | Project website | Poster | Presentation (15min)\nMeta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies."
  },
  {
    "objectID": "projects/index.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "href": "projects/index.html#bayesflow-amortized-bayesian-workflows-with-neural-networks",
    "title": "Projects",
    "section": "BayesFlow: Amortized Bayesian Workflows With Neural Networks",
    "text": "BayesFlow: Amortized Bayesian Workflows With Neural Networks\n\nSoftware Paper | Documentation | BayesFlow Forums (new!)\nBayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized."
  },
  {
    "objectID": "projects/index.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "href": "projects/index.html#jana-jointly-amortized-neural-approximation-of-complex-bayesian-models",
    "title": "Projects",
    "section": "JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models",
    "text": "JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models\n\nPaper (UAI 2023) | Python library\nJANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation."
  },
  {
    "objectID": "projects/index.html#sbi-model-misspecification",
    "href": "projects/index.html#sbi-model-misspecification",
    "title": "Projects",
    "section": "Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks",
    "text": "Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks\n\nPaper (GCPR 2023, Best Paper Honorable Mention) | Code | Poster Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic domain shifts during inference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I'm Marvin.",
    "section": "",
    "text": "I like using deep neural networks to solve hard problems in probabilistic machine learning. I am a Computer Scientist and a Psychologist.\n            \n            \n            I am a final-year PhD student at the European Laboratory for Learning and Intelligent Systems (ELLIS) supervised by Paul Bürkner, Stefan Radev, and Aki Vehtari. My main affiliation is the Cluster of Excellence SimTech at the University of Stuttgart, Germany.\n            \n            \n            \n            JOB MARKET: I am looking for machine learning positions in Europe (or remote) starting in early 2026. My current interests revolve around deep learning, probabilistic ML, and ML4Science. I'm curious about new topics as well. I spend a considerable amount of my time programming in Python (JAX, PyTorch, TensorFlow, or Keras). Check out my CV (Link) and drop me a message if there might be a fit: mail.marvinschmitt@gmail.com"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Hi, I'm Marvin.",
    "section": "News",
    "text": "News\n\nNovember 9, 2024: My work just reached 100 citations on Google Scholar. It might be small for many others, but it means a lot to me. I started my PhD with no research experience, and I’ve learned so much ever since. Thank you to my mentors, collaborators, and everyone who engaged with my work 🧡\nSeptember 25, 2024: Our paper on consistency models for simulation-based inference (Link) has been accepted at NeurIPS 2024!\nJuly 8, 2024: I started a YouTube channel! The videos will broadly fall into these categories: Early career research based on my experience, technical machine learning topics, applied programming in R and Python. In my first video, I talk about my background and my plans for the channel: \nJune 6, 2024: My guest episode on the Learning Bayesian Statistics podcast is live. We had a great conversation about my research on amortized Bayesian inference with neural networks. Here’s the announcement post on Twitter and LinkedIn.\nApril 3, 2024: Just finished recording an episode of the Learning Bayesian Statistics podcast. The episode will air later this summer. Thanks Alex Andorra for having me as a guest!\nMarch 1, 2024: I’m spending 6 months as a visiting researcher in the Probabilistic ML group at Aalto University in Finland, hosted by Aki Vehtari.\nJanuary 23, 2024: I will be at Bayes on the Beach 2024, Australia, in February. I will give a talk about reliable amortized Bayesian inference with neural networks and co-lead a workshop on amortized Bayesian workflows.\nDecember 2023: We launched the BayesFlow Forums! The BayesFlow Forums provide a community for asking and answering questions about all aspects of BayesFlow and amortized Bayesian workflows in general. Hop over and join the discussion!\nOctober 11, 2023: In our recent preprint, we make simulation-based inference more data-efficient by leveraging self-consistency properties of the Bayesian joint model. You find the preprint on arXiv. Update: Published in the NeurIPS UniReps workshop.\nSeptember 22, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been awarded the DAGM GCPR Honorable Mention at this year’s German Conference on Pattern Recognition. Huge thanks to my great co-authors!\nSeptember 6, 2023: Our paper “Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks” has been accepted with oral presentation at the German Conference for Pattern Recognition 2023. Learn more at the project website.\nJune 21, 2023: I will be at the ELLIS Doctoral Symposium 2023 in Helsinki from August 28 to September 1, 2023. Absolutely thrilled to meet other PhD students and researchers and talk about machine learning research in beautiful Helsinki!\nMay 9, 2023: Our paper “JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models” has been accepted to UAI 2023 (Uncertainty in AI). Check out the project page to find out more!\nMay 1, 2023: Our paper “Association between vitamin D status and eryptosis–results from the German National Cohort Study” has been published in Annals of Hematology. A great collaboration among twelve researchers from Martin-Luther-University Halle-Wittenberg, Cluster of Excellence SimTech (University of Stuttgart) and University of Hohenheim. Here’s a link to the paper.\nApril 12, 2023: I have recorded a 15min presentation about our paper on Meta-Uncertainty in Bayesian Model Comparison (AISTATS 2023). Check it out: \nMarch 29, 2023: I’ll be presenting our paper on Meta-Uncertainty in Bayesian Model Comparison at AISTATS 2023 from April 25-27 in Valencia, Spain. You find links to the paper and code on my Projects page or on the dedicated paper website. If you see me around at AISTATS, let’s chat!"
  },
  {
    "objectID": "index.html#featured-blog-posts",
    "href": "index.html#featured-blog-posts",
    "title": "Hi, I'm Marvin.",
    "section": "Featured Blog Posts",
    "text": "Featured Blog Posts\n\n\n\n\n\n\n\n\n\n\nStructured Language Generation with Outlines in R\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI started a YouTube channel!\n\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Your Website with Quarto: Complete Tutorial and Template\n\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/laptop-mic-obs/index.html",
    "href": "blog/laptop-mic-obs/index.html",
    "title": "Make your built-in laptop mic sound good in OBS",
    "section": "",
    "text": "I recorded a short tutorial video, where I walk you through the steps to improve the audio quality of your built-in laptop microphone in OBS. Here’s the video:\n\nThe video is unlisted because it doesn’t fit in the usual content of my channel. If you’re interested in a polished version of the video for the main channel, let me know and I’ll add it to my content list. If you have any questions or suggestions, feel free to reach out – always happy to help."
  },
  {
    "objectID": "blog/functional-programming-r-basics/index.html",
    "href": "blog/functional-programming-r-basics/index.html",
    "title": "Functional Programming in R, Part 1: Basics",
    "section": "",
    "text": "What is functional programming?\nReady for a quick dive into the wonderful world of functional programming? Whether you’re a seasoned programmer or just starting your journey with R for some data science project, getting a grasp on functional programming will not only add a new weapon to your data science arsenal but also make your code more efficient and readable. So, without further ado, let’s get started!\nFunctional programming, put simply, is a programming style that mimics mathematical functions. You might be thinking, “Isn’t all programming based on functions?” Well, yes, but functional programming takes this concept to the next level. It’s all about structuring your code as a series of reusable, interconnected functions, each doing one specific task and doing it well. In this blog post, we’ll focus on functions as arguments to other functions.\nNow, why should you, an R programmer, care about this? Well, R might not be a pure functional programming language, but it does offer strong functional programming capabilities. Mastering them can make your code more effective and attractive. Plus, functional programming principles can help us eliminate redundant code and make our scripts more maintainable.\nIt’s a bit like building with Lego blocks. Each block (or function) has its own shape and purpose, and you can connect them in various ways to create whatever you want. And the best part? You can disassemble and reassemble them without affecting the individual blocks!\n\n\nBasics in R: Passing functions as arguments\nLet’s start with a powerful feature that many data science workflows heavily benefit from: passing functions as arguments to other functions. This concept might seem a bit tricky at first, but once you get the hang of it, you’ll see how it adds a whole new level of flexibility and efficiency to your code.\nIn R, functions are “first-class citizens”, meaning they can be treated just like any other object or data type. They can be assigned to variables, stored in lists, and most importantly for our current discussion, passed as arguments to other functions. This feature allows us to create more general, reusable functions and reduce code repetition.\n\n\nExample 1: Using the apply() function\nOne of the most common examples of passing a function as an argument is using the apply() function in R. The apply() applies a function to the rows or columns of a matrix or data frame. Here’s how it works.\nWe will first set the stage and initialize a \\(2 \\times 3\\) matrix m:\n\n# Create a matrix\nm = matrix(c(1, 2, 3, 4, 5, 6), nrow = 2)\n\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nNow, suppose we want to calculate the sum of each row in m. We could do that manually via:\n\n# Initialize a vector to store the row sums\nrow_sums = numeric(nrow(m))\n\nfor (i in seq_len(nrow(m))){\n  row_sums[i] = sum(m[i, ])\n}\n\nprint(row_sums)\n\n[1]  9 12\n\n\nBut this code is pretty clunky for such a simple task and, more importantly, hard to maintain. What if we wanted to compute the sum of each column? We would need to\n\nrename row_sums to col_nums both at the initialization and within the for loop;\ncarefully adjust the iterator i so that it iterates over [1, ..., ncol(m)]; and finally\nadjust the matrix slicing to m[, i].\n\nI don’t know about you, but I would probably miss at least one of those adjustments.\n\n\n\nExample: Using the apply function to calculate row sums.\n\n\nSo let’s rewrite row_sums as a one-liner via functional programming. We use the function apply(X, MARGIN, FUN), which works as follows:\n\nX: The object that we operate on. In our case, we want to apply some function on parts of the matrix m, so X = m.\nMARGIN: You can select whether you want to apply the function to each row (MARGIN = 1), to each column (MARGIN=2), or to rows and columns combined (MARGIN=c(1,2)). We want to compute the sum of each row, so we choose MARGIN = 1.\nFUN: The actual function that you want to apply across the rows or columns. In our case, it’s the sum function.\n\n\n# Apply the sum function to the rows of the matrix\nrow_sums = apply(m, MARGIN=1, FUN=sum)\n\nprint(row_sums)\n\n[1]  9 12\n\n\nIf you want to take your first steps towards functional programming, go ahead and try those two exercises.\n\n\n\n\n\n\nExercise 1\n\n\n\nChange the example code so that it calculates the column sums of the matrix m instead of the row sums. The answer should be [3, 7, 11].\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nChange the example code so that calculates the mean of each column in m instead of the sum. The answer should be [1.5, 3.5, 5.5].\n\n\n\n\nExample 2: Additional arguments to FUN – ignore missing values\nSuppose we have a matrix of data, and some of the values are missing, as represented by NA:\n\n# Create a matrix\nm = matrix(c(1, 2, NA, 4, 5, 6), nrow = 2)\n\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1   NA    5\n[2,]    2    4    6\n\n\nIf we simply execute the code from the previous example, we will get NA in the row with a missing value:\n\n# Compute row sums (without handling NA)\nrow_sums = apply(m, MARGIN=1, FUN=sum)\nprint(row_sums)\n\n[1] NA 12\n\n\nIf we think about it, it’s not surprising: Behind the scenes, we execute sum(c(1, NA, 5)) and that’s NA. If you are a bit into data science with R, you might know that sum(c(1, NA, 5), na.rm=TRUE) would ignore the NA value and return the sum of the non-NA values, which is 6. So we need a way to funnel na.rm through the apply function into sum. Lucky for us, there is a straightforward way. We can simply add any additional arguments in our call to apply as follows:\n\n# Compute the sum of each row, removing NA values\nrow_sums &lt;- apply(m, 1, sum, na.rm=TRUE)\n\nprint(row_sums)\n\n[1]  6 12\n\n\nIn this example, we’re passing the sum function as an argument to the apply function as usual. But we’re also passing na.rm=TRUE as an additional argument to the sum function. This means that when apply calls the sum function for each row, it’s actually calling sum(row, na.rm=TRUE). Perfect!\n\n\nExample 3: Custom functions – computing error metrics\nSuppose you are writing some machine learning algorithm that aims to predict the true value for four test cases. But your algorithm tries to quantify some notion of uncertainty around that prediction. Therefore, it outputs three values for each case:\n\n# Define a true vector and a predicted matrix\ntrue &lt;- c(1, 3, 2, 2)\npredicted &lt;- matrix(c(1, 2, 3, 2, 3, 4, 1, 3, 3, 2, 2, 2), nrow = 4)\n\nfor (i in seq_along(true)){\n  cat(\"True:\", true[i], \"| predictions:\", predicted[i, ], \"\\n\")\n}\n\nTrue: 1 | predictions: 1 3 3 \nTrue: 3 | predictions: 2 4 2 \nTrue: 2 | predictions: 3 1 2 \nTrue: 2 | predictions: 2 3 2 \n\n\nNow, you want to quantify the error for each case with some metric to know how good the set of predictions is for each case. In this example, we’ll first use the Mean Squared Error (MSE) as our metric. MSE is a common metric for regression problems, measuring the average squared differences between the true and predicted values. Here, it will quantify the error between the set of predictions \\(\\hat{y}_1, \\ldots, \\hat{y}_N\\) and the true value \\(y^*\\) for a single case, \\(MSE=\\frac{1}{N}\\sum\\limits_{i=1}^N(y^*-\\hat{y}_i)^2\\).\n\n# Define a function to compute the Mean Squared Error\nmse &lt;- function(true, predicted) {\n  return (mean((true - predicted) ** 2))\n}\n\nIf we want to know how well the algorithm predicts the first test case, we can simply call:\n\nmse(true[1], predicted[1, ])\n\n[1] 2.666667\n\n\nThis is the prediction error for the first case, and we could compute such an error for every case in the test set. Now, let’s create a function that takes the entire true vector, the entire predicted matrix, and an error metric function as arguments. It will apply the error metric function to the true vector and each column of the predicted matrix. It might seem like a bit of an overkill for now, but it will be worth it very soon:\n\n# Create a function that computes the error between a true vector and each column of a predicted matrix\ncompute_errors &lt;- function(true, predicted, error_func) {\n  # Initialize a vector to store the errors\n  errors &lt;- numeric(ncol(predicted))\n  \n  # For each column in the predicted matrix\n  for (i in seq_len(ncol(predicted))) {\n    # Compute the error between the true vector and the current column\n    errors[i] &lt;- error_func(true, predicted[,i])\n  }\n  \n  return(errors)\n}\n\nFinally, let’s use our compute_error function to compute the MSE between the true vector and each column of the prediction matrix:\n\n# Use the custom function to compute the MSEs\nerrors &lt;- compute_errors(true, predicted, error_func=mse)\n\nprint(errors)\n\n[1] 0.50 1.75 1.25\n\n\nIn this example, we compute the MSE between the true vector and each column of the predicted matrix. We achieve this by passing the mse function as an argument to the compute_errors function. This demonstrates how passing functions as arguments can make your code more flexible and modular, enabling you to easily switch between different error metrics. It was a bit of upfront work, but it reduces the actual call to compute the errors down to a single line of code.\nBut this functional programming paradigm really shines when we want to replace the error function that we apply! Let’s compute the Mean Average Error (MAE) instead of the MSE. No problem with our functional programming solution. After all, the concrete error function just an argument to our compute_errors wrapper that implements all the logic. We just define the mae function \\(\\frac{1}{N}\\sum\\limits_{i=1}^N | y^*-\\hat{y}_i|\\) and plug it into the compute_errors wrapper:\n\n# Define a function to compute the Mean Absolute Error\nmae &lt;- function(true, predicted) {\n  return (mean(abs(true - predicted)))\n}\n\n# Use the custom function to compute the MAEs\nerrors &lt;- compute_errors(true, predicted, error_func=mae)\n\nprint(errors)\n\n[1] 0.50 1.25 0.75\n\n\nBoom, we compute the MAE between each case from the true vector and each corresponding set of predictions from the predicted matrix. All we had to do is pass the mae function as an argument to the compute_errors function. Most importantly, we did not need to make any changes to the compute_errors function – instead, we just provided a drop-in replacement for the error_func that compute_errors applies internally.\n\n\n\n\n\n\nExercise 3\n\n\n\nAdjust the MAE example code so that NA values are ignored and the mean is computed with respect to all non-NA values.\n\n\n\n\nNext up: partial functions with {purrr}\nFor me personally, one of the most fascinating aspects of functional programming in R is the concept of ‘partial functions’. This concept allows us to create new functions from existing ones by pre-filling some of the arguments. Think of it as a customizable tool that you can tweak to fit your specific needs.\nIn part 2 of this blog post series, we’ll be taking a closer look at partial functions in R. We’ll understand how they work, why they’re useful, and how you can use them to make your R code even more efficient, elegant, and re-usable.\nSo, stay tuned for the next blog post. If you want to be notified via email when the next blog post is published, consider subscribing to my slow mailing list The Training Loop below.\nCheers!"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html",
    "href": "blog/ggsimplex-prerelease/index.html",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "",
    "text": "Our latest manuscript Meta-Uncertainty in Bayesian Model Comparison has been accepted to AISTATS 2023! That’s a reason to celebrate 🎉 At the same time, this means that a lot of people hear about the project and some are interested in the code. So far, so good. Of course, we have released the software code for the paper in a public GitHub repository. But there is one problem. The fancy triangular density plots use an internal and highly experimental ggplot extension: {ggsimplex} (GitHub source)\nWe planned to release {ggsimplex} at some point next year, but now we feel like at least the experimental version should be made public for the AISTATS paper. This blog post gives a brief rundown of the package."
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#what-is-a-simplex",
    "href": "blog/ggsimplex-prerelease/index.html#what-is-a-simplex",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "What is a simplex?",
    "text": "What is a simplex?\nA simplex is the generalization of a triangle to \\(n\\) dimensions. In this blog post, we are exclusively interested in 2-simplices, that is: triangles.\n\n\n\n\n\n\nFigure 1: Examples of some \\(n\\)-simplices: point, line, triangle, and tetrahedron.\n\n\n\nThe mathematical formulation for everything within a (\\(J-1\\)) simplex \\(\\Delta\\) is pretty uncanny:\n\\[\n            \\Delta =\n            \\left\\{\n                x\\in\\mathbb{R}^J: x = \\sum\\limits_{j=1}^J\\pi_j v_j\n                \\quad\n                \\text{with}\n                \\quad\n                0\\leq \\pi_j \\leq 1, \\sum\\limits_{j=1}^J\\pi_j=1\n            \\right\\}.\n\\]\nThis sounds overly complex, but it basically just says “a vector of numbers that is non-negative, sums to one, and lies within the vertices”.\nBut why would we care about simplices? It turns out that some interesting data types actually live in simplices (plural of simplex). One example are compartmental data: By volume, the dry air in Earth’s atmosphere is about 78 percent nitrogen, 21 percent oxygen, and 1 percent argon (source). These percentages sum to one and can be visualized in a simplex. The main reason for me to develop the {ggsimplex}` page are so-called posterior model probabilities.\nSay we compare \\(J\\) different candidate models \\(M_1, \\ldots, M_J\\) in a Bayesian setting and we want to assess their fit to a data set \\(y\\). Then, we can use Bayes’ formula (through some intricate computational methods) to estimate the probability of each model given the observed data, namely the posterior model probability (PMP) for each model \\(M_j\\): \\(p(M_j\\,|\\,y)\\). By definition, the PMPs sum to one: \\(\\sum_{j=1}^J p(M_j\\,|\\, y)=1\\). So PMPs on a fixed data set are data that live within a simplex 🤩 Enough preface, let’s jump in."
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#installation",
    "href": "blog/ggsimplex-prerelease/index.html#installation",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Installation",
    "text": "Installation\nYou can install {ggsimplex} from GitHub via the {devtools} package, and then load it along with {ggplot2}:\n\ndevtools::install_github('marvinschmitt/ggsimplex')\nlibrary(ggsimplex)\nlibrary(ggplot2)"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#canvas-setup",
    "href": "blog/ggsimplex-prerelease/index.html#canvas-setup",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Canvas setup",
    "text": "Canvas setup\nIn the experimental stage, the first step for every simplex plot is setting up the canvas by setting the aspect ratio, clearing the frame, and drawing the triangular border:\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas()"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#point-plots",
    "href": "blog/ggsimplex-prerelease/index.html#point-plots",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Point plots",
    "text": "Point plots\nWe sample some data from a distribution with support on the simplex, such as the Dirichlet distribution. This is conveniently implemented in {brms} through brms::rdirichlet. We need some more preprocessing and bind the simplex data into a single column pmp. The make_list_column function implements the list column conversion.\n\nlibrary(brms)\ndata = rdirichlet(n = 100, alpha = c(1,2,3))\ndata = as.data.frame(data)\ncolnames(data) = c(\"pmp_1\", \"pmp_2\", \"pmp_3\")\n\ndata$pmp = with(data, make_list_column(pmp_1, pmp_2, pmp_3))\n\nNow we use geom_simplex_point and pass the data into the pmp asthetic (short for “posterior model probabilities”). The other arguments of geom_simplex_point are basically identical to the standard geom_point – we can use arguments like size, color, and alpha.\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas() + \n  geom_simplex_point(data = data, aes(pmp = pmp),\n                     size = 0.7, color = \"firebrick\", alpha = 0.8)"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#density-plots",
    "href": "blog/ggsimplex-prerelease/index.html#density-plots",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Density plots",
    "text": "Density plots\nNow we want to plot an analytic density which is defined on the simplex. Let’s take the Dirichlet density from the example above with \\(\\alpha=(1,2,3)\\). We prepare the data in a data frame, which might seem overly complex at this point – but it will come in handy when we want to take advantage of advanced ggplot features such as faceting.\n\ndf_dirichlet = data.frame(true_model = 1)\ndf_dirichlet$Alpha = list(c(1, 2, 3))\n\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas() + \n  stat_simplex_density(data=df_dirichlet, fun = ddirichlet,\n                       args = alist(Alpha=Alpha))\n\n\n\n\n\n\n\n\nThe modular structure of ggplot allows us to plot the scatter on top of the density plot:\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas() + \n  stat_simplex_density(data=df_dirichlet, fun = ddirichlet,\n                       args = alist(Alpha=Alpha)) +\n  geom_simplex_point(data = data, aes(pmp = pmp),\n                   size = 0.7, color = \"firebrick\", alpha = 0.8)"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#faceting",
    "href": "blog/ggsimplex-prerelease/index.html#faceting",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Faceting",
    "text": "Faceting\nOne core idea of the Meta-Uncertainty Framework lies in analyzing the model-implied posterior model probability distributions (= pushforward of the prior predictive) of different data generating models. So let’s look at simplex data from three differently parameterized Dirichlet distributions. We simulate the data into separate data frames, then bind them and create the list column pmp. Then, we save the parameters of the underlying Dirichlet distributions in the data frame df_dirichlet again so that we can generate density plots as above.1\n1 The data preparation could be way more generic and elegant, but for the sake of clearness, this blog post sacrifices some elegance 😉\nalpha_1 = c(1, 2, 3)\ndata_1 = data.frame(true_model = 1, rdirichlet(n = 100, alpha = alpha_1))\n\nalpha_2 = c(2, 5, 1)\ndata_2 = data.frame(true_model = 2, rdirichlet(n = 100, alpha = alpha_2))\n\nalpha_3 = c(4, 2, 2)\ndata_3 = data.frame(true_model = 3, rdirichlet(n = 100, alpha = alpha_3))\n\ndata = rbind(data_1, data_2, data_3)\ncolnames(data) = c(\"true_model\", \"pmp_1\", \"pmp_2\", \"pmp_3\")\ndata$pmp = with(data, make_list_column(pmp_1, pmp_2, pmp_3))\n\ndf_dirichlet = data.frame(true_model = 1:3)\n\ndf_dirichlet$Alpha = list(alpha_1, alpha_2, alpha_3)\n\nNow that the column true_model identifies the different data generating processes, we can simply add the facet_grid and the ggplot magic happens:\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas() + \n  stat_simplex_density(data=df_dirichlet, fun = ddirichlet,\n                       args = alist(Alpha=Alpha)) +\n  geom_simplex_point(data = data, aes(pmp = pmp),\n                   size = 0.7, color = \"firebrick\", alpha = 0.3) +\n  facet_grid(~true_model, labeller=label_both)\n\n\n\n\n\n\n\n\n\nOther distributions\nThe stat_simplex_density function is designed to accept a density function and a list of parameters. So we can simply pass the density function of a logistic Normal distribution, as implemented in brms::dlogistic_normal. The logistic normal distribution is parameterized by \\(\\mu\\) and \\(\\Sigma\\), so we pass these parameters in a list column, grouped by the true model because we want a nice facet plot.\n\nmu_1 = c(0, 0)\nSigma_1 = matrix(c(1, 0, 0, 1), nrow=2, byrow=TRUE)\ndata_1 = data.frame(true_model = 1, \n                    rlogistic_normal(n = 100, mu = mu_1, Sigma = Sigma_1))\n\nmu_2 = c(0, 0)\nSigma_2 = matrix(c(0.3, 0, 0, 0.3), nrow=2, byrow=TRUE)\ndata_2 = data.frame(true_model = 2, \n                    rlogistic_normal(n = 100, mu = mu_2, Sigma = Sigma_2))\n\nmu_3 = c(0, 0)\nSigma_3 = matrix(c(0.5, 0.3, 0.3, 1), nrow=2, byrow=TRUE)\ndata_3 = data.frame(true_model = 3, \n                    rlogistic_normal(n = 100, mu = mu_3, Sigma = Sigma_3))\n\ndata = rbind(data_1, data_2, data_3)\ncolnames(data) = c(\"true_model\", \"pmp_1\", \"pmp_2\", \"pmp_3\")\ndata$pmp = with(data, make_list_column(pmp_1, pmp_2, pmp_3))\n\ndf_logistic_normal = data.frame(true_model = 1:3)\ndf_logistic_normal$mu = list(mu_1, mu_2, mu_3)\ndf_logistic_normal$Sigma = list(Sigma_1, Sigma_2, Sigma_3)\n\nThe actual plotting is straightforward again. We tell stat_simplex_density that we want a dlogistic_normal density function and pass the mu and Sigma columns of the df_logistic_normal data frame containing the parameters for each facet.\n\nggplot() +\n  coord_fixed(ratio=1, xlim=c(0, 1), ylim=c(0, 1))+\n  theme_void() +\n  geom_simplex_canvas() + \n  stat_simplex_density(data=df_logistic_normal, fun = dlogistic_normal,\n                       args = alist(mu = mu, Sigma = Sigma)) +\n  geom_simplex_point(data = data, aes(pmp = pmp),\n                   size = 0.7, color = \"firebrick\", alpha = 0.3) +\n  facet_grid(~true_model, labeller=label_both)"
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#issue-tracker",
    "href": "blog/ggsimplex-prerelease/index.html#issue-tracker",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Issue Tracker",
    "text": "Issue Tracker\nPlease keep in mind that the {ggsimplex} package is in a very early stage. If it wasn’t for the AISTATS paper on Meta-Uncertainty in Bayesian Model Comparison, I would not have pre-released this immature package. That being said, bug reports and feature requests are always welcome at the GitHub Issues Page. All issue reports are appreciated and will be considered for the actual package release. Please do not expect them to be fixed anytime soon, though."
  },
  {
    "objectID": "blog/ggsimplex-prerelease/index.html#outlook",
    "href": "blog/ggsimplex-prerelease/index.html#outlook",
    "title": "Pre-releasing the {ggsimplex} R package",
    "section": "Outlook",
    "text": "Outlook\nThe {ggsimplex} package is designed as a ggplot extension. This means that it follows the ggplot interfaces, which makes it compatible with other ggplot-based packages such as {gganimate}. More on that in another post when {ggsimplex} development has advanced further.\nThis blog post covered the elementary pre-beta functionality of the {ggsimplex} package. Expect large (and breaking) changes to happen in the future. Thank you for reading! If you enjoy content like this, follow me on Twitter (@MarvinSchmittML) or LinkedIn (Marvin Schmitt).\nCheers,\nMarvin"
  },
  {
    "objectID": "blog/hello-world/index.html",
    "href": "blog/hello-world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "(Photo by Markus Spiske)\nFor the past years, my homepage was just a placeholder. It served a single purpose: People who enter the domain of my email address in their browser should not look at some weird empty page. So here we go!\nThanks to the incredible Quarto project it was really a matter of a few hours to set everything up. Thank you!\nBTW: The top banner is a stock photo. If I ever produce code that looks anything like this dense mess, please remind me to quit programming forever.\nAs pretty much every aspiring early career researcher, I am obviously planning to publish valuable, insightful, and thought-provoking blog posts regularly. See you tomorrow next week in a month at some time for the first actual post. Cheers!"
  },
  {
    "objectID": "blog/4-realizations-from-atomic-habits/index.html",
    "href": "blog/4-realizations-from-atomic-habits/index.html",
    "title": "4 Realizations from Atomic Habits",
    "section": "",
    "text": "About Atomic Habits\nIn his best-selling book Atomic Habits*, the author James Clear argues that the key to making meaningful progress in our lives is not about making grand, sweeping changes, but rather about building small, incremental habits that compound over time. With his scientific approach to habit formation and practical, actionable advice, James Clear shows readers how to break bad habits and form good ones to achieve their goals and improve their lives. Whether you’re looking to improve your health, increase your wealth, or achieve personal growth, Atomic Habits* offers a simple yet powerful framework for making positive changes that stick.\nIt has gained international attention: At the time of writing this blog post, over 8 million copies have been sold. The title “Atomic Habits” immediately drew my attention. It communicates the idea that small, incremental changes can have a significant impact on our lives. Much like atoms, which are the basic building blocks of matter.1 These small habits, or “atomic habits”, can either work for or against us, depending on whether they are positive or negative. By focusing on building positive atomic habits, we can make significant progress towards our goals and create a better life for ourselves. “Atomic” captures the very essence of behavior change from a psychological perspective:\n1 dear physicist reading this: ‘basic building blocks’ is meant figuratively. Please resist the urge to correct this ;)\nAtoms are tiny. Behavior change needs to happen in tiny bits to be sustainable.\nAtoms are crucial building blocks of the world around us. Likewise, habits are crucial building blocks of human behavior.\n\nBoth aspects surface throughout the book in different forms, in different contexts, and with different implications. Yet, if I were to explain the very core idea of the book to someone in under 30 seconds, I would build on peoples’ pre-existing intuition of what makes something atomic.\nIn this blog post, I am in a lucky position to have more than 30 seconds. That’s why I want to share my personal four realizations from the book Atomic Habits.\n\n\nRealization 1: Four Laws of Behavior Change\nJames Clear builds his method on an adapted version of classical behavioral Psychology:\n\nCue → Craving → Action → Reward\n\nEach step is a cogwheel to increase the chance to perform a good habit. The framework for working with these cogwheels are the Four Laws of Behavior Change. Each law can be inverted to decrease the odds of a bad habit, but more on that later.\n\n\n\nStage\nLaw: Good Habits\nInverted Law: Bad Habits\n\n\n\n\n1: Cue\nMake it visible\nMake it invisible\n\n\n2: Craving\nMake it attractive\nMake it unattractive\n\n\n3: Action\nMake it easy\nMake it difficult\n\n\n4: Reward\nMake it satisfying\nMake it unsatisfying\n\n\n\nThese laws form the core framework of the book. Each law can be used to facilitate good habits and make you stop bad habits. It’s straightforward to think of examples for each law:\n\nLaw 1: Make it visible\nI want to read more. That’s why I try to keep a book right at my bedside table. When I go to bed at night and see a book there, I always pick it up and read a bit.\nImplementing the habit of reading each night literally made me buy the book Atomic Habits*. Some time ago, I attended a software engineering workshop and spent a night at an AirBnB. On my way to the workshop, I realized that I forgot the book I was currently reading at home. I went to an English book store and Atomic Habits* was the first book that I grabbed – what a fortunate coincidence!\n\n\nLaw 2: Make it attractive\nLet’s say you want to start exercising regularly, but you struggle to find the motivation to do so. One way to make exercise more attractive is to find a form of physical activity that you genuinely enjoy, such as dancing, swimming, or playing a sport. By choosing an activity that you find enjoyable, you are more likely to look forward to your workouts and be motivated to do them consistently. You could also try finding a workout buddy or joining a class to make the experience more social and enjoyable. By making exercise more attractive, you are more likely to stick with it and make it a regular part of your routine.\n\nDeep Dive: The attractiveness of some behavior refers to how tempting the behavior is when looking forward to it. That’s not the same as satisfying behavior. Consider the example of regular exercise: The problem is that most people (me included) just don’t exercise in the first place because it is unattractive in anticipation. However, once I get in my running shoes and start exercising, I enjoy it. It is unattractive in anticipation but satisfying in action.\n\n\n\nLaw 3: Make it easy\nA linter is a tool that checks your code for syntax errors and style issues, such as indentation errors, unnecessary white space, or naming conventions. By using a linter, you can catch and fix these issues before you run your code, which can save you time and frustration. This can be particularly helpful when working on a team, as it can help to ensure that your code is consistent with the project’s style guidelines.\nIn this way, using a linter can be seen as a way of applying the third law of behavior change (make it easy) to your Python coding. By automating the process of checking your code for errors and style issues, a linter makes it easier for you to write clean and organized Python code. This can save you time and effort and help you avoid common coding mistakes. By making it easy to write clean Python code, you are more likely to build the habit of producing high-quality code that is easy to read and maintain.\n\n\nLaw 4: Make it satisfying\nI want to reduce avoidable expenses. However, not buying that croissant at the bakery does not bring any joy by itself: I am not a frugalist who finds the actual act of not spending money attractive. To make not spending money satisfying, I transfer the money that I did not spend on that croissant to a savings account. The money in this savings account is dedicated to non-essential stuff which I really want to purchase (but won’t buy right away because I’m trying to be financially responsible).\nIt turns out that the money from unbought stuff adds up. For instance, I recently purchased an awesome outdoor pizza oven from the accumulated money in this savings account. In hindsight, that pizza oven is way more satisfying than all the croissants, coffees, and chocolate bars that I did not buy. And that makes not buying that croissant incredibly satisfying for me – because I replace this action with the action “transfer the croissant money to the savings account”.\n\nDeep Dive: “Transfer the croissant money to the savings account” is again a habit that is governed by the Four Laws. For instance, Law 2 (make it easy): If wiring money requires going to the bank and filling out a transaction form (difficult), I would probably stop doing it after a couple of times. Instead, I simply use my mobile phone for the transaction and it takes less than a minute (easy).\n\n\n\n\nRealization 2: Real-life situations are inverse\nInteresting real-life scenarios are a little more complicated – they are often inverse to the actual theory. Let me explain what I mean by this cryptic sentence. What we did so far is take a Law (e.g., make it visible) and think about examples of where we might apply that strategy. Real-life usually works the opposite way: We begin the thought process with a good habit we want to implement (or a bad habit we want to stop). Then, we need to figure out a strategy to achieve this intention: Should we first try to make this bad habit invisible, unattractive, difficult, or unsatisfying? We definitely should not try to do all of the above at once, because this might be too big of a change and we probably get back to normal after a week. So we need a method to identify potential angles to tackle our intention. James Clear proposes to first analyze the habit of interest and identify its cue, craving, action, and reward. Let’s apply this framework to a habit of mine that I want to stop:\nChecking my emails disrupts my flow state when I am programming. That’s why I deliberately don’t use a proper e-mail program (like Outlook) on my work laptop. Instead, I need to manually log in to each of my mail accounts to check my emails. That’s just enough of a barrier for me to not check my emails every other minute.\n\n\nRealization 3: Habits do not stack, habits compound\nJames Clear makes a fair point when he claims that habits are the compound interest of behavior change.\n\nWait, like compound interest? What is this, a pyramid scheme?\n\n\n\n\n\n\n\n\n\n\nHabits can be thought of as the compound interest of behavior change because, like compound interest, they have a cumulative effect that grows over time. When you develop a new habit, it may seem small and insignificant at first. However, as you continue to practice that habit day after day, it gradually becomes stronger and more ingrained in your routine. When we perform a behavior consistently, it becomes a habit, which means we can do it automatically without having to think about it. This frees up our mental energy and allows us to focus on other things. The impact of habits is often not immediately noticeable, but over time they can have a significant effect on our lives.\nLet’s say you want to improve your physical fitness and develop the habit of exercising regularly. At first, you may struggle to find the motivation to exercise and may not see significant progress in your fitness level. However, as you continue to exercise consistently, you will begin to see small improvements in your strength and endurance.\nAs you continue to exercise consistently over time, those small improvements will compound and you will see more significant progress in your fitness level. You may find that you can run longer distances, lift heavier weights, or perform other physical activities with greater ease. These improvements will encourage you to continue exercising regularly, and the habit will become stronger and more ingrained in your routine.\nIn this way, the habit of exercising regularly can be seen as the compound interest of behavior change. By starting small and building upon that habit over time, you can make significant progress in your physical fitness and improve your overall health and well-being.\n\n\nRealization 4: Many habits fly under the radar\nHabits are deeply routed and fully automatic. More often than not, we are completely unaware of our habits. For instance, I was on vacation recently. During my vacation, I wanted to resist the urge to read any work-related emails or messages on my phone.\nTo make the habit of reading work-related messages invisible (1st Law), I just deleted Slack from my phone. That one was easy. In contrast, deleting the email app from my phone wasn’t as easy because (i) I needed access to private emails; and (ii) re-configuring my entire email app after the vacation would take way too long. That’s why I simply moved the email app from my phone’s fast access bar into some junkyard folder that’s crammed with unused apps.\nI was surprised by what happened during my vacation: Whenever I unlocked my phone, my thumb would automatically gravitate to the fast access bar where the email app used to be! Moving the app to another location made me realize how deeply automated the habit of opening the email app upon unlocking my phone had become over the past years. The brief moment of irritation when my brain realized that the app is no longer at its usual place was enough to achieve a crucial step to change bad habits: It made the bad habit visible – I was finally aware of it!\n\nPsychologists would say that the behavior has become salient.\n\nReturning from my vacation, my email app did not make it back to the fast access bar. Whenever I want to check my emails on my mobile phone, I now need to navigate into that dusty jukyard folder This makes the habit of checking emails on my phone just a little more invisible and just a little more difficult. I appreciate that.\n\n\nConclusion\nReading Atomic Habits* started as a work trip accident because I forgot my other book at home. I must admit that it was a fortunate accident There are two more closing remarks I want to make.\nFirst, the book matched well with my own previous experience on behavior change. I could see many parallels to what has worked well for me in the past, and found those things that went poorly for me in the past in the book’s counter-examples. At the same time, the book gave me a proper vocabulary for all my implicit knowledge and experience on behavior change. Now, I have the right words to express what I have been experiencing all along. This makes the topic of behavior change much more actionable, and that’s basis for achieving sustainable change.\nSecond, Atomic Habits* is a rather light read, but it does contain a lot of information, and many claims are backed by scientific references. I would often find myself thinking “well, that claim is far-fetched”, just to see that James Clear did actually provide an academic paper to support that claim in the appendix. As a scientist, I appreciate this a lot, and I know that this is not common practice in this genre of books.\nIn conclusion, Atomic Habits* by James Clear offers a simple yet powerful framework for making positive changes that stick. By focusing on building small, incremental habits that compound over time, we can make meaningful progress toward our goals and improve our lives. Clear’s scientific approach to habit formation and practical, actionable advice make it an invaluable resource for anyone looking to break bad habits and form good ones.\nTo take action based on the ideas in this book, consider the following steps:\n\nIdentify a specific goal or habit that you want to work on.\nMake a plan for how you will build this habit, using the four laws of behavior change: make it obvious, make it attractive, make it easy, and make it satisfying.\nTrack your progress and celebrate your successes along the way.\nSeek out resources and support to help you stay motivated and on track.\n\nBy following these steps, you can begin to build the atomic habits that will help you achieve your goals and create a better life for yourself.\nI can highly recommend reading Atomic Habits* to find out more about the patterns in your habits. You might just get some inspiration about how to stop this one habit that you have been trying to get rid of for years.\n\nAffiliate link disclaimer: This post may contain affiliate links. When you buy a product after clicking the affiliate link, I might get a small commission from the purchase you do. To make affiliate links clearly visible, the actual link is followed by *."
  },
  {
    "objectID": "blog/scientists-should-have-a-website/index.html",
    "href": "blog/scientists-should-have-a-website/index.html",
    "title": "Every Scientist Should Have a Website",
    "section": "",
    "text": "Introduction\nIn today’s fast-paced academic landscape, establishing a strong online presence is crucial for scientists to showcase their research, connect with peers, and build their professional brand. Don’t get me wrong, a digital publication list is nice and cool; but having a personal website means much more than a mere collection of papers. It is a great tool to highlight our achievements, foster collaborations, and inspire others on a global scale. Let’s take a look at five compelling reasons why you should consider creating your own website as well.\n\n\nReason 1: Showcase Your Work\nImagine having a central hub where you can effectively present your research achievements, publications, and expertise to the scientific community and beyond. A personal website provides precisely that. By curating your work in one place, you can stand out in the competitive scientific landscape and make it easier for others to discover and appreciate your valuable contributions. This platform offers a great view of your research journey.\n\n\nReason 2: Expand Your Network\nCollaboration is key to scientific progress. Your personal website acts as a gateway, connecting you with fellow scientists, potential collaborators, and industry professionals. It opens doors to new opportunities and facilitates the expansion of your professional network. By providing contact information and writing about your research interests, you can attract like-minded individuals who share your passion and expertise. Through your website, you can foster collaborations, exchange ideas, and forge valuable connections.\n\n\nReason 3: Have Control Over Your Content\nIn today’s digital age, maintaining control over your online presence is essential. While social media platforms and other online channels offer great reach, they often come with limitations and constraints. A personal website grants you the freedom to share your research findings, insights, and perspectives on your own terms. You are not bound by character counts or algorithmic filters. This level of control allows you to maintain a professional online persona while freely expressing your thoughts and ideas.\n\n\nReason 4: Reach an Audience\nWhile traditional scientific publications are the currency of academia, their reach can be limited. A personal website breaks down these barriers and makes you reach a global audience. By sharing your research with the world, you can engage with science enthusiasts, inspire others, and make a lasting impact beyond academic papers. Whether your goal is to communicate complex scientific concepts to a broader audience or connect with fellow researchers on a global scale, a personal website will bring you one step closer to your goal.\n\n\nReason 5: Enhance Opportunities\nA personal website is a way for potential employers and academic institutions to find out about your work, achievements, and contributions. It offers them a comprehensive understanding of your expertise, research interests, and the impact of your work. By showcasing your accomplishments and highlighting your unique perspectives, you can enhance your chances of exciting career opportunities.\n\n\nHow to Start Today\nSo why wait? Take the leap and create your personal website today! To assist you, I have prepared a free tutorial and template to get you started on the right foot:\n\n\n\nConclusion\nHaving a personal website empowers you to showcase your work, expand your professional network, maintain control over your content, reach a global audience, and enhance your career opportunities. Remember: Your research is valuable, and you can create a personal website to put yourself out there and share your knowledge with others in no time."
  },
  {
    "objectID": "blog/outlines-r/index.html",
    "href": "blog/outlines-r/index.html",
    "title": "Structured Language Generation with Outlines in R",
    "section": "",
    "text": "Python is a great Swiss army knife for many tasks, especially when it comes to deep learning these days. However, many statisticians and data scientists prefer working in R. In this short blog post, we’ll use the reticulate package to work with Python code inside R. This lets us use the great outlines package by dottxt, and OpenAI’s GPT-4o as a language model backend."
  },
  {
    "objectID": "blog/outlines-r/index.html#load-reticulate-and-set-up-the-environment",
    "href": "blog/outlines-r/index.html#load-reticulate-and-set-up-the-environment",
    "title": "Structured Language Generation with Outlines in R",
    "section": "Load Reticulate and Set Up the Environment",
    "text": "Load Reticulate and Set Up the Environment\nLoad the reticulate library to interface between R and Python, and specify the conda environment that contains the necessary Python packages (outlines, openai and tiktoken in my case).\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.4.1\n\nuse_condaenv(\"/Users/marvin/miniforge3/envs/outlines_py310\", required = TRUE)\nos &lt;- import(\"os\")\noutlines &lt;- import(\"outlines\")"
  },
  {
    "objectID": "blog/outlines-r/index.html#set-up-the-openai-model",
    "href": "blog/outlines-r/index.html#set-up-the-openai-model",
    "title": "Structured Language Generation with Outlines in R",
    "section": "Set Up the OpenAI Model",
    "text": "Set Up the OpenAI Model\nSet up the OpenAI model using the outlines package. You should not write your API key directly in the code. Instead, we use an environment variable which we set in the terminal before running the R script. This ensures that you don’t accidentally leak your secret API key.\n\napi_key &lt;- Sys.getenv(\"OPENAI_API_KEY\")\nmodel &lt;- outlines$models$openai(\"gpt-4o\", api_key = api_key)"
  },
  {
    "objectID": "blog/outlines-r/index.html#generate-a-response",
    "href": "blog/outlines-r/index.html#generate-a-response",
    "title": "Structured Language Generation with Outlines in R",
    "section": "Generate a Response",
    "text": "Generate a Response\nNow we’ll use the language model to answer a question, and restrict the answer to a choice from multiple options. For demonstration purposes, let’s see whether GPT-4 can answer a basic question about Bayesian statistics.\n\nchoices = c(\"Prior\", \"Likelihood\", \"Marginal Likelihood\", \"Evidence\", \"Posterior\")\ngenerator &lt;- outlines$generate$choice(model, choices)\n\nresult &lt;- generator(\"In a Bayesian model, what do we call the probability distribution of parameters given the data?\")\nprint(result)\n\n[1] \"Posterior\"\n\n\nLet’s try another more technical question, this time about the choice of a suitable likelihood for count data.\n\nchoices = c(\"Gaussian\", \"Poisson\", \"Negative-Binomial\", \"Gamma\")\ngenerator &lt;- outlines$generate$choice(model, choices)\n\nresult &lt;- generator(\"We have a Bayesian model for count data $y$. The data $y$ is lower-bounded at zero, can take on integer values, and is probably overdispersed. The most suitable likelihood is \")\nprint(result)\n\n[1] \"Negative-Binomial\""
  },
  {
    "objectID": "blog/outlines-r/index.html#next-steps",
    "href": "blog/outlines-r/index.html#next-steps",
    "title": "Structured Language Generation with Outlines in R",
    "section": "Next Steps",
    "text": "Next Steps\nIn this demonstrator, we used reticulate as a simple bridge to call Python packages from within R. As a next steps, you can try other generation schemes (not just multiple choice) or build more complex pipelines. Also, Outlines really shines if you use it with a local open source LLM, so you should try that as well."
  },
  {
    "objectID": "blog/mac-email-shortcut/index.html",
    "href": "blog/mac-email-shortcut/index.html",
    "title": "Quick hack: Mac email shortcut",
    "section": "",
    "text": "👆 That shortcut has saved me a bunch of time, and many many typos in email addresses. Whenever I type @@g, my computer automatically changes that to mail.marvinschmitt@gmail.com.\nI have a @@_ shortcut for every email address I use. In this short blog post, I’ll give you a quick tutorial on how to set up these custom shortcuts on a Mac. It’s probably gonna take you less than five minutes, and save you way more than that in return.\n⭐ Sounds like a plan? Let’s go! ⭐\n\nGo to System Preferences and select the Keyboard submenu.\n\nNavigate to the Text tab and hit the + button to add a new shortcut.\n\nAdd the Shortcut to the “Replace” column, and the full email address to the “With” column. I recommend to prefix the shortcut with @@ because it makes it practically impossible to trigger the shortcut accidentally. Yet, it’s reminiscent of an email address.\n\nRepeat, profit.\n\n\n🏁 That’s it! 🥳\nYou can use this method to create many more custom shortcuts, but my primary use case are all my email addresses. What are your ideas for custom shortcuts? I’d love to hear about them in the comment section below 👇\nPS: This method has one crucial caveat. When I’m typing on someone else’s computer, I regularly type @@g on laser speed, followed by a brief moment of confusion, and a brief moment of disappointment when the shortcut doesn’t fire. But well, He giveth and he taketh away 🤷\n---\nThumbnail photo by Maksim Goncharenok: https://www.pexels.com/photo/gold-letter-y-on-black-background-5605061/"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  }
]